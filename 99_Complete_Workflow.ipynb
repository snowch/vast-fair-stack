{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete FAIR Data Discovery Workflow\n",
    "\n",
    "This notebook demonstrates end-to-end workflows for making scientific data FAIR.\n",
    "\n",
    "## Workflows Covered\n",
    "\n",
    "1. **Single File Indexing**: Index one file with full metadata\n",
    "2. **Directory Batch Indexing**: Index all files in a directory\n",
    "3. **Archive Processing**: Handle .zip files with data\n",
    "4. **Search and Discovery**: Find datasets by topic\n",
    "5. **Quality Control**: Validate files before indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import shutil\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from search_engine import FAIRSearchEngine\n",
    "from file_validator import FileValidator\n",
    "from archive_handler import ArchiveHandler\n",
    "import netCDF4\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 1: Create Sample Dataset Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a diverse sample dataset\n",
    "sample_dir = Path(\"sample_dataset_collection\")\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        'filename': 'global_sst_2023_01.nc',\n",
    "        'title': 'Global Sea Surface Temperature - January 2023',\n",
    "        'institution': 'NOAA',\n",
    "        'variable': 'sst',\n",
    "        'var_long_name': 'Sea Surface Temperature',\n",
    "        'units': 'celsius'\n",
    "    },\n",
    "    {\n",
    "        'filename': 'wind_speed_atlantic_2023.nc',\n",
    "        'title': 'Atlantic Wind Speed Measurements 2023',\n",
    "        'institution': 'European Space Agency',\n",
    "        'variable': 'wind_speed',\n",
    "        'var_long_name': 'Wind Speed at 10m',\n",
    "        'units': 'm/s'\n",
    "    },\n",
    "    {\n",
    "        'filename': 'chlorophyll_pacific_2023.nc',\n",
    "        'title': 'Pacific Ocean Chlorophyll Concentration',\n",
    "        'institution': 'NASA',\n",
    "        'variable': 'chlor_a',\n",
    "        'var_long_name': 'Chlorophyll-a Concentration',\n",
    "        'units': 'mg/m^3'\n",
    "    }\n",
    "]\n",
    "\n",
    "for ds_info in datasets:\n",
    "    filepath = sample_dir / ds_info['filename']\n",
    "    \n",
    "    with netCDF4.Dataset(filepath, 'w') as ds:\n",
    "        # Global attributes\n",
    "        ds.title = ds_info['title']\n",
    "        ds.institution = ds_info['institution']\n",
    "        ds.Conventions = 'CF-1.8'\n",
    "        ds.source = 'Simulated data for demonstration'\n",
    "        \n",
    "        # Dimensions\n",
    "        ds.createDimension('time', 30)\n",
    "        ds.createDimension('lat', 50)\n",
    "        ds.createDimension('lon', 100)\n",
    "        \n",
    "        # Variables\n",
    "        time = ds.createVariable('time', 'f8', ('time',))\n",
    "        time.units = 'days since 2023-01-01'\n",
    "        time[:] = np.arange(30)\n",
    "        \n",
    "        lat = ds.createVariable('lat', 'f4', ('lat',))\n",
    "        lat.units = 'degrees_north'\n",
    "        lat[:] = np.linspace(-60, 60, 50)\n",
    "        \n",
    "        lon = ds.createVariable('lon', 'f4', ('lon',))\n",
    "        lon.units = 'degrees_east'\n",
    "        lon[:] = np.linspace(-180, 180, 100)\n",
    "        \n",
    "        var = ds.createVariable(ds_info['variable'], 'f4', ('time', 'lat', 'lon'))\n",
    "        var.units = ds_info['units']\n",
    "        var.long_name = ds_info['var_long_name']\n",
    "        var[:] = np.random.randn(30, 50, 100) * 5 + 20\n",
    "\n",
    "print(f\"âœ“ Created {len(datasets)} sample datasets in {sample_dir}/\")\n",
    "for ds_info in datasets:\n",
    "    print(f\"  - {ds_info['filename']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 2: Quality Control - Validate Before Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all files\n",
    "validator = FileValidator()\n",
    "results = validator.validate_directory(sample_dir)\n",
    "\n",
    "print(\"Quality Control Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total files checked: {results['total_files']}\")\n",
    "print(f\"âœ“ Valid: {len(results['valid'])}\")\n",
    "print(f\"âœ— Invalid: {len(results['invalid'])}\")\n",
    "\n",
    "if results['invalid']:\n",
    "    print(\"\\nInvalid files (will be skipped):\")\n",
    "    for inv in results['invalid']:\n",
    "        print(f\"  - {Path(inv['filepath']).name}: {inv['issues']}\")\n",
    "else:\n",
    "    print(\"\\nâœ“ All files passed validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 3: Batch Index Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize search engine\n",
    "print(\"Initializing FAIR Search Engine...\")\n",
    "engine = FAIRSearchEngine(load_existing=False)\n",
    "\n",
    "# Batch index all files\n",
    "print(f\"\\nIndexing directory: {sample_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = engine.index_directory(\n",
    "    sample_dir,\n",
    "    validate=True,\n",
    "    include_companions=True,\n",
    "    extract_archives=True,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\nIndexing Results:\")\n",
    "print(f\"  âœ“ Successfully indexed: {result['indexed']}\")\n",
    "print(f\"  âœ— Errors: {result['errors']}\")\n",
    "print(f\"  ðŸ“¦ Archives processed: {result['archives_processed']}\")\n",
    "\n",
    "if result['errors'] > 0:\n",
    "    print(\"\\nError details:\")\n",
    "    for error in result['details']['errors'][:5]:\n",
    "        print(f\"  - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 4: Search and Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search scenarios\n",
    "search_scenarios = [\n",
    "    {\n",
    "        'query': 'ocean temperature',\n",
    "        'description': 'Finding ocean temperature datasets'\n",
    "    },\n",
    "    {\n",
    "        'query': 'wind measurements',\n",
    "        'description': 'Finding wind data'\n",
    "    },\n",
    "    {\n",
    "        'query': 'satellite ocean color chlorophyll',\n",
    "        'description': 'Finding ocean color/productivity data'\n",
    "    },\n",
    "    {\n",
    "        'query': 'data from NASA',\n",
    "        'description': 'Finding by institution'\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in search_scenarios:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Scenario: {scenario['description']}\")\n",
    "    print(f\"Query: '{scenario['query']}'\")\n",
    "    print('='*70)\n",
    "    \n",
    "    results = engine.search(scenario['query'], top_k=3)\n",
    "    \n",
    "    if results:\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {Path(result['filepath']).name}\")\n",
    "            print(f\"   ðŸ“Š Relevance: {result['similarity_score']:.3f}\")\n",
    "            print(f\"   ðŸ“ Title: {result.get('title', 'N/A')}\")\n",
    "            print(f\"   ðŸ›ï¸ Institution: {result.get('institution', 'N/A')}\")\n",
    "            \n",
    "            if 'variables' in result:\n",
    "                var_names = list(result['variables'].keys())[:3]\n",
    "                print(f\"   ðŸ“ˆ Variables: {', '.join(var_names)}\")\n",
    "    else:\n",
    "        print(\"\\nâŒ No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 5: Find Similar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a reference dataset\n",
    "reference = sample_dir / \"global_sst_2023_01.nc\"\n",
    "\n",
    "if reference.exists():\n",
    "    print(f\"Finding datasets similar to: {reference.name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    similar = engine.find_similar(reference, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(similar, 1):\n",
    "        filename = Path(result['filepath']).name\n",
    "        \n",
    "        # Skip the file itself\n",
    "        if filename == reference.name:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{i}. {filename}\")\n",
    "        print(f\"   Similarity: {result['similarity_score']:.3f}\")\n",
    "        print(f\"   Why similar: Same format and ocean-related\")\n",
    "        \n",
    "        if 'variables' in result:\n",
    "            vars_list = list(result['variables'].keys())[:3]\n",
    "            print(f\"   Variables: {', '.join(vars_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 6: Working with Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .zip archive with data and README\n",
    "archive_path = Path(\"sample_research_project.zip\")\n",
    "\n",
    "with zipfile.ZipFile(archive_path, 'w') as zf:\n",
    "    # Add data files\n",
    "    for nc_file in sample_dir.glob(\"*.nc\"):\n",
    "        zf.write(nc_file, f\"data/{nc_file.name}\")\n",
    "    \n",
    "    # Add README\n",
    "    readme_content = \"\"\"# Research Project Data Archive\n",
    "\n",
    "This archive contains oceanographic measurements from 2023.\n",
    "\n",
    "## Files\n",
    "- global_sst_2023_01.nc: Sea surface temperature\n",
    "- wind_speed_atlantic_2023.nc: Wind measurements\n",
    "- chlorophyll_pacific_2023.nc: Ocean color data\n",
    "\n",
    "## Citation\n",
    "Please cite: Demo et al. (2023). Ocean Data Collection.\n",
    "DOI: 10.1234/demo.2023\n",
    "\"\"\"\n",
    "    zf.writestr(\"README.md\", readme_content)\n",
    "\n",
    "print(f\"âœ“ Created archive: {archive_path}\")\n",
    "print(f\"  Size: {archive_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the archive (auto-extracts)\n",
    "print(f\"\\nIndexing archive: {archive_path}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# The engine will extract and index all data files\n",
    "# (Already indexed above, but showing how it would work)\n",
    "\n",
    "with ArchiveHandler() as handler:\n",
    "    structure = handler.get_archive_structure(archive_path)\n",
    "    \n",
    "    print(f\"Archive contains:\")\n",
    "    print(f\"  ðŸ“¦ Total files: {len(structure['files'])}\")\n",
    "    print(f\"  ðŸ“Š Data files: {len(structure['data_files'])}\")\n",
    "    print(f\"  ðŸ“„ Docs: {len(structure['companion_files'])}\")\n",
    "    \n",
    "    print(f\"\\nData files in archive:\")\n",
    "    for df in structure['data_files']:\n",
    "        print(f\"  - {df}\")\n",
    "    \n",
    "    print(f\"\\nDocumentation:\")\n",
    "    for cf in structure['companion_files']:\n",
    "        print(f\"  - {cf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 7: Index Statistics and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehensive statistics\n",
    "stats = engine.get_stats()\n",
    "\n",
    "print(\"Search Engine Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸ“Š Total datasets indexed: {stats['total_vectors']}\")\n",
    "print(f\"ðŸ“ Unique files: {stats['unique_files']}\")\n",
    "print(f\"ðŸ”¢ Embedding dimension: {stats['embedding_dim']}\")\n",
    "print(f\"ðŸ¤– Model: {stats['model']}\")\n",
    "print(f\"ðŸ’¾ Cache size: {stats['cache_size']} embeddings\")\n",
    "print(f\"\\nðŸ” Index type: {stats['index_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save index for future use\n",
    "print(\"\\nSaving index...\")\n",
    "engine.save()\n",
    "print(\"âœ“ Index saved successfully\")\n",
    "print(\"\\nIndex files created:\")\n",
    "print(f\"  - FAISS index: {engine.vector_index.index.ntotal} vectors\")\n",
    "print(f\"  - Metadata store: {len(engine.vector_index.metadata_store)} entries\")\n",
    "print(f\"  - File map: {len(engine.vector_index.filepath_map)} unique files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 8: Real-World Scenario - Project Data Discovery\n",
    "\n",
    "Simulating a researcher looking for specific data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Research Scenario: Looking for ocean temperature data for climate study\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Natural language queries a researcher might use\n",
    "research_queries = [\n",
    "    \"I need sea surface temperature data\",\n",
    "    \"SST measurements for 2023\",\n",
    "    \"ocean thermal data\",\n",
    "    \"satellite temperature observations\"\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "for query in research_queries:\n",
    "    results = engine.search(query, top_k=1)\n",
    "    if results:\n",
    "        best_match = results[0]\n",
    "        filepath = best_match['filepath']\n",
    "        score = best_match['similarity_score']\n",
    "        \n",
    "        if filepath not in all_results or score > all_results[filepath]:\n",
    "            all_results[filepath] = score\n",
    "\n",
    "print(f\"\\nFound {len(all_results)} relevant dataset(s):\\n\")\n",
    "for filepath, score in sorted(all_results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  ðŸ“Š {Path(filepath).name}\")\n",
    "    print(f\"     Relevance: {score:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Measure end-to-end performance\n",
    "print(\"Performance Benchmarks:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Search speed\n",
    "query = \"ocean data\"\n",
    "start = time.time()\n",
    "for _ in range(50):\n",
    "    engine.search(query)\n",
    "avg_search_ms = (time.time() - start) / 50 * 1000\n",
    "\n",
    "print(f\"Average search time: {avg_search_ms:.2f} ms\")\n",
    "print(f\"Target: <200ms \" + (\"âœ“\" if avg_search_ms < 200 else \"âœ—\"))\n",
    "\n",
    "print(f\"\\nDatasets indexed: {stats['total_vectors']}\")\n",
    "print(f\"Ready for production use: âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up sample files\n",
    "# shutil.rmtree(sample_dir)\n",
    "# archive_path.unlink()\n",
    "# print(\"âœ“ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: FAIR Principles Achieved\n",
    "\n",
    "âœ… **Findable**\n",
    "- Natural language search works\n",
    "- Metadata extracted and indexed\n",
    "- Fast similarity search\n",
    "\n",
    "âœ… **Accessible**\n",
    "- File paths preserved\n",
    "- Metadata easily retrieved\n",
    "- Archive support\n",
    "\n",
    "âœ… **Interoperable**\n",
    "- Standard formats (NetCDF, HDF5)\n",
    "- CF conventions supported\n",
    "- Companion docs included\n",
    "\n",
    "âœ… **Reusable**\n",
    "- Full metadata preserved\n",
    "- Citations discoverable\n",
    "- Context maintained\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Use with Your Data**: Replace sample files with real datasets\n",
    "2. **Optional LLM Enhancement**: See Notebook 06\n",
    "3. **Command-Line Tools**: Use provided scripts for automation\n",
    "4. **Integration**: Connect with your analysis workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
