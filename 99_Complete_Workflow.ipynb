{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete FAIR Data Discovery Workflow\n",
    "\n",
    "This notebook demonstrates end-to-end workflows for making scientific data FAIR.\n",
    "\n",
    "## Workflows Covered\n",
    "\n",
    "1. **Single File Indexing**: Index one file with full metadata\n",
    "2. **Directory Batch Indexing**: Index all files in a directory\n",
    "3. **Archive Processing**: Handle .zip files with data\n",
    "4. **Search and Discovery**: Find datasets by topic\n",
    "5. **Quality Control**: Validate files before indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import shutil\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from search_engine import FAIRSearchEngine\n",
    "from file_validator import FileValidator\n",
    "from archive_handler import ArchiveHandler\n",
    "import netCDF4\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 1: Create Sample Dataset Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 3 sample datasets in sample_dataset_collection/\n",
      "  - global_sst_2023_01.nc\n",
      "  - wind_speed_atlantic_2023.nc\n",
      "  - chlorophyll_pacific_2023.nc\n"
     ]
    }
   ],
   "source": [
    "# Create a diverse sample dataset\n",
    "sample_dir = Path(\"sample_dataset_collection\")\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        'filename': 'global_sst_2023_01.nc',\n",
    "        'title': 'Global Sea Surface Temperature - January 2023',\n",
    "        'institution': 'NOAA',\n",
    "        'variable': 'sst',\n",
    "        'var_long_name': 'Sea Surface Temperature',\n",
    "        'units': 'celsius'\n",
    "    },\n",
    "    {\n",
    "        'filename': 'wind_speed_atlantic_2023.nc',\n",
    "        'title': 'Atlantic Wind Speed Measurements 2023',\n",
    "        'institution': 'European Space Agency',\n",
    "        'variable': 'wind_speed',\n",
    "        'var_long_name': 'Wind Speed at 10m',\n",
    "        'units': 'm/s'\n",
    "    },\n",
    "    {\n",
    "        'filename': 'chlorophyll_pacific_2023.nc',\n",
    "        'title': 'Pacific Ocean Chlorophyll Concentration',\n",
    "        'institution': 'NASA',\n",
    "        'variable': 'chlor_a',\n",
    "        'var_long_name': 'Chlorophyll-a Concentration',\n",
    "        'units': 'mg/m^3'\n",
    "    }\n",
    "]\n",
    "\n",
    "for ds_info in datasets:\n",
    "    filepath = sample_dir / ds_info['filename']\n",
    "    \n",
    "    with netCDF4.Dataset(filepath, 'w') as ds:\n",
    "        # Global attributes\n",
    "        ds.title = ds_info['title']\n",
    "        ds.institution = ds_info['institution']\n",
    "        ds.Conventions = 'CF-1.8'\n",
    "        ds.source = 'Simulated data for demonstration'\n",
    "        \n",
    "        # Dimensions\n",
    "        ds.createDimension('time', 30)\n",
    "        ds.createDimension('lat', 50)\n",
    "        ds.createDimension('lon', 100)\n",
    "        \n",
    "        # Variables\n",
    "        time = ds.createVariable('time', 'f8', ('time',))\n",
    "        time.units = 'days since 2023-01-01'\n",
    "        time[:] = np.arange(30)\n",
    "        \n",
    "        lat = ds.createVariable('lat', 'f4', ('lat',))\n",
    "        lat.units = 'degrees_north'\n",
    "        lat[:] = np.linspace(-60, 60, 50)\n",
    "        \n",
    "        lon = ds.createVariable('lon', 'f4', ('lon',))\n",
    "        lon.units = 'degrees_east'\n",
    "        lon[:] = np.linspace(-180, 180, 100)\n",
    "        \n",
    "        var = ds.createVariable(ds_info['variable'], 'f4', ('time', 'lat', 'lon'))\n",
    "        var.units = ds_info['units']\n",
    "        var.long_name = ds_info['var_long_name']\n",
    "        var[:] = np.random.randn(30, 50, 100) * 5 + 20\n",
    "\n",
    "print(f\"✓ Created {len(datasets)} sample datasets in {sample_dir}/\")\n",
    "for ds_info in datasets:\n",
    "    print(f\"  - {ds_info['filename']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 2: Quality Control - Validate Before Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Control Results:\n",
      "============================================================\n",
      "Total files checked: 3\n",
      "✓ Valid: 3\n",
      "✗ Invalid: 0\n",
      "\n",
      "✓ All files passed validation!\n"
     ]
    }
   ],
   "source": [
    "# Validate all files\n",
    "validator = FileValidator()\n",
    "results = validator.validate_directory(sample_dir)\n",
    "\n",
    "print(\"Quality Control Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total files checked: {results['total_files']}\")\n",
    "print(f\"✓ Valid: {len(results['valid'])}\")\n",
    "print(f\"✗ Invalid: {len(results['invalid'])}\")\n",
    "\n",
    "if results['invalid']:\n",
    "    print(\"\\nInvalid files (will be skipped):\")\n",
    "    for inv in results['invalid']:\n",
    "        print(f\"  - {Path(inv['filepath']).name}: {inv['issues']}\")\n",
    "else:\n",
    "    print(\"\\n✓ All files passed validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 3: Batch Index Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing FAIR Search Engine...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Model loaded. Embedding dimension: 384\n",
      "Loaded embedding cache: 23 entries\n",
      "Creating new index...\n",
      "Initialized FAISS index (dim=384)\n",
      "\n",
      "Indexing directory: sample_dataset_collection\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing files: 100%|█| 3/3 [00:00<00:00,  4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing Results:\n",
      "  ✓ Successfully indexed: 3\n",
      "  ✗ Errors: 0\n",
      "  📦 Archives processed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize search engine\n",
    "print(\"Initializing FAIR Search Engine...\")\n",
    "engine = FAIRSearchEngine(load_existing=False)\n",
    "\n",
    "# Batch index all files\n",
    "print(f\"\\nIndexing directory: {sample_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = engine.index_directory(\n",
    "    sample_dir,\n",
    "    validate=True,\n",
    "    include_companions=True,\n",
    "    extract_archives=True,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\nIndexing Results:\")\n",
    "print(f\"  ✓ Successfully indexed: {result['indexed']}\")\n",
    "print(f\"  ✗ Errors: {result['errors']}\")\n",
    "print(f\"  📦 Archives processed: {result['archives_processed']}\")\n",
    "\n",
    "if result['errors'] > 0:\n",
    "    print(\"\\nError details:\")\n",
    "    for error in result['details']['errors'][:5]:\n",
    "        print(f\"  - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 4: Search and Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Scenario: Finding ocean temperature datasets\n",
      "Query: 'ocean temperature'\n",
      "======================================================================\n",
      "\n",
      "1. global_sst_2023_01.nc\n",
      "   📊 Relevance: 0.488\n",
      "   📝 Title: Global Sea Surface Temperature - January 2023\n",
      "   🏛️ Institution: NOAA\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "2. chlorophyll_pacific_2023.nc\n",
      "   📊 Relevance: 0.404\n",
      "   📝 Title: Pacific Ocean Chlorophyll Concentration\n",
      "   🏛️ Institution: NASA\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "3. wind_speed_atlantic_2023.nc\n",
      "   📊 Relevance: 0.289\n",
      "   📝 Title: Atlantic Wind Speed Measurements 2023\n",
      "   🏛️ Institution: European Space Agency\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "======================================================================\n",
      "Scenario: Finding wind data\n",
      "Query: 'wind measurements'\n",
      "======================================================================\n",
      "\n",
      "1. wind_speed_atlantic_2023.nc\n",
      "   📊 Relevance: 0.514\n",
      "   📝 Title: Atlantic Wind Speed Measurements 2023\n",
      "   🏛️ Institution: European Space Agency\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "2. global_sst_2023_01.nc\n",
      "   📊 Relevance: 0.252\n",
      "   📝 Title: Global Sea Surface Temperature - January 2023\n",
      "   🏛️ Institution: NOAA\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "3. chlorophyll_pacific_2023.nc\n",
      "   📊 Relevance: 0.244\n",
      "   📝 Title: Pacific Ocean Chlorophyll Concentration\n",
      "   🏛️ Institution: NASA\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "======================================================================\n",
      "Scenario: Finding ocean color/productivity data\n",
      "Query: 'satellite ocean color chlorophyll'\n",
      "======================================================================\n",
      "\n",
      "1. chlorophyll_pacific_2023.nc\n",
      "   📊 Relevance: 0.659\n",
      "   📝 Title: Pacific Ocean Chlorophyll Concentration\n",
      "   🏛️ Institution: NASA\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "2. global_sst_2023_01.nc\n",
      "   📊 Relevance: 0.286\n",
      "   📝 Title: Global Sea Surface Temperature - January 2023\n",
      "   🏛️ Institution: NOAA\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "3. wind_speed_atlantic_2023.nc\n",
      "   📊 Relevance: 0.219\n",
      "   📝 Title: Atlantic Wind Speed Measurements 2023\n",
      "   🏛️ Institution: European Space Agency\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "======================================================================\n",
      "Scenario: Finding by institution\n",
      "Query: 'data from NASA'\n",
      "======================================================================\n",
      "\n",
      "1. wind_speed_atlantic_2023.nc\n",
      "   📊 Relevance: 0.384\n",
      "   📝 Title: Atlantic Wind Speed Measurements 2023\n",
      "   🏛️ Institution: European Space Agency\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "2. global_sst_2023_01.nc\n",
      "   📊 Relevance: 0.374\n",
      "   📝 Title: Global Sea Surface Temperature - January 2023\n",
      "   🏛️ Institution: NOAA\n",
      "   📈 Variables: time, lat, lon\n",
      "\n",
      "3. chlorophyll_pacific_2023.nc\n",
      "   📊 Relevance: 0.301\n",
      "   📝 Title: Pacific Ocean Chlorophyll Concentration\n",
      "   🏛️ Institution: NASA\n",
      "   📈 Variables: time, lat, lon\n"
     ]
    }
   ],
   "source": [
    "# Define search scenarios\n",
    "search_scenarios = [\n",
    "    {\n",
    "        'query': 'ocean temperature',\n",
    "        'description': 'Finding ocean temperature datasets'\n",
    "    },\n",
    "    {\n",
    "        'query': 'wind measurements',\n",
    "        'description': 'Finding wind data'\n",
    "    },\n",
    "    {\n",
    "        'query': 'satellite ocean color chlorophyll',\n",
    "        'description': 'Finding ocean color/productivity data'\n",
    "    },\n",
    "    {\n",
    "        'query': 'data from NASA',\n",
    "        'description': 'Finding by institution'\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in search_scenarios:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Scenario: {scenario['description']}\")\n",
    "    print(f\"Query: '{scenario['query']}'\")\n",
    "    print('='*70)\n",
    "    \n",
    "    results = engine.search(scenario['query'], top_k=3)\n",
    "    \n",
    "    if results:\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {Path(result['filepath']).name}\")\n",
    "            print(f\"   📊 Relevance: {result['similarity_score']:.3f}\")\n",
    "            print(f\"   📝 Title: {result.get('title', 'N/A')}\")\n",
    "            print(f\"   🏛️ Institution: {result.get('institution', 'N/A')}\")\n",
    "            \n",
    "            if 'variables' in result:\n",
    "                var_names = list(result['variables'].keys())[:3]\n",
    "                print(f\"   📈 Variables: {', '.join(var_names)}\")\n",
    "    else:\n",
    "        print(\"\\n❌ No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 5: Find Similar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding datasets similar to: global_sst_2023_01.nc\n",
      "============================================================\n",
      "\n",
      "2. wind_speed_atlantic_2023.nc\n",
      "   Similarity: 0.746\n",
      "   Why similar: Same format and ocean-related\n",
      "   Variables: time, lat, lon\n",
      "\n",
      "3. chlorophyll_pacific_2023.nc\n",
      "   Similarity: 0.595\n",
      "   Why similar: Same format and ocean-related\n",
      "   Variables: time, lat, lon\n"
     ]
    }
   ],
   "source": [
    "# Pick a reference dataset\n",
    "reference = sample_dir / \"global_sst_2023_01.nc\"\n",
    "\n",
    "if reference.exists():\n",
    "    print(f\"Finding datasets similar to: {reference.name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    similar = engine.find_similar(reference, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(similar, 1):\n",
    "        filename = Path(result['filepath']).name\n",
    "        \n",
    "        # Skip the file itself\n",
    "        if filename == reference.name:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{i}. {filename}\")\n",
    "        print(f\"   Similarity: {result['similarity_score']:.3f}\")\n",
    "        print(f\"   Why similar: Same format and ocean-related\")\n",
    "        \n",
    "        if 'variables' in result:\n",
    "            vars_list = list(result['variables'].keys())[:3]\n",
    "            print(f\"   Variables: {', '.join(vars_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 6: Working with Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created archive: sample_research_project.zip\n",
      "  Size: 1788.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Create a .zip archive with data and README\n",
    "archive_path = Path(\"sample_research_project.zip\")\n",
    "\n",
    "with zipfile.ZipFile(archive_path, 'w') as zf:\n",
    "    # Add data files\n",
    "    for nc_file in sample_dir.glob(\"*.nc\"):\n",
    "        zf.write(nc_file, f\"data/{nc_file.name}\")\n",
    "    \n",
    "    # Add README\n",
    "    readme_content = \"\"\"# Research Project Data Archive\n",
    "\n",
    "This archive contains oceanographic measurements from 2023.\n",
    "\n",
    "## Files\n",
    "- global_sst_2023_01.nc: Sea surface temperature\n",
    "- wind_speed_atlantic_2023.nc: Wind measurements\n",
    "- chlorophyll_pacific_2023.nc: Ocean color data\n",
    "\n",
    "## Citation\n",
    "Please cite: Demo et al. (2023). Ocean Data Collection.\n",
    "DOI: 10.1234/demo.2023\n",
    "\"\"\"\n",
    "    zf.writestr(\"README.md\", readme_content)\n",
    "\n",
    "print(f\"✓ Created archive: {archive_path}\")\n",
    "print(f\"  Size: {archive_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing archive: sample_research_project.zip\n",
      "============================================================\n",
      "Archive contains:\n",
      "  📦 Total files: 4\n",
      "  📊 Data files: 3\n",
      "  📄 Docs: 1\n",
      "\n",
      "Data files in archive:\n",
      "  - data/chlorophyll_pacific_2023.nc\n",
      "  - data/wind_speed_atlantic_2023.nc\n",
      "  - data/global_sst_2023_01.nc\n",
      "\n",
      "Documentation:\n",
      "  - README.md\n"
     ]
    }
   ],
   "source": [
    "# Index the archive (auto-extracts)\n",
    "print(f\"\\nIndexing archive: {archive_path}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# The engine will extract and index all data files\n",
    "# (Already indexed above, but showing how it would work)\n",
    "\n",
    "with ArchiveHandler() as handler:\n",
    "    structure = handler.get_archive_structure(archive_path)\n",
    "    \n",
    "    print(f\"Archive contains:\")\n",
    "    print(f\"  📦 Total files: {len(structure['files'])}\")\n",
    "    print(f\"  📊 Data files: {len(structure['data_files'])}\")\n",
    "    print(f\"  📄 Docs: {len(structure['companion_files'])}\")\n",
    "    \n",
    "    print(f\"\\nData files in archive:\")\n",
    "    for df in structure['data_files']:\n",
    "        print(f\"  - {df}\")\n",
    "    \n",
    "    print(f\"\\nDocumentation:\")\n",
    "    for cf in structure['companion_files']:\n",
    "        print(f\"  - {cf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 7: Index Statistics and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Engine Statistics:\n",
      "============================================================\n",
      "📊 Total datasets indexed: 3\n",
      "📁 Unique files: 3\n",
      "🔢 Embedding dimension: 384\n",
      "🤖 Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "💾 Cache size: 30 embeddings\n",
      "\n",
      "🔍 Index type: IndexFlatIP\n"
     ]
    }
   ],
   "source": [
    "# Get comprehensive statistics\n",
    "stats = engine.get_stats()\n",
    "\n",
    "print(\"Search Engine Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📊 Total datasets indexed: {stats['total_vectors']}\")\n",
    "print(f\"📁 Unique files: {stats['unique_files']}\")\n",
    "print(f\"🔢 Embedding dimension: {stats['embedding_dim']}\")\n",
    "print(f\"🤖 Model: {stats['model']}\")\n",
    "print(f\"💾 Cache size: {stats['cache_size']} embeddings\")\n",
    "print(f\"\\n🔍 Index type: {stats['index_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving index...\n",
      "Index saved to /home/vastdata/vast-fair-stack/lib/indexes/faiss_index.bin\n",
      "✓ Index saved successfully\n",
      "\n",
      "Index files created:\n",
      "  - FAISS index: 3 vectors\n",
      "  - Metadata store: 3 entries\n",
      "  - File map: 3 unique files\n"
     ]
    }
   ],
   "source": [
    "# Save index for future use\n",
    "print(\"\\nSaving index...\")\n",
    "engine.save()\n",
    "print(\"✓ Index saved successfully\")\n",
    "print(\"\\nIndex files created:\")\n",
    "print(f\"  - FAISS index: {engine.vector_index.index.ntotal} vectors\")\n",
    "print(f\"  - Metadata store: {len(engine.vector_index.metadata_store)} entries\")\n",
    "print(f\"  - File map: {len(engine.vector_index.filepath_map)} unique files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow 8: Real-World Scenario - Project Data Discovery\n",
    "\n",
    "Simulating a researcher looking for specific data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Scenario: Looking for ocean temperature data for climate study\n",
      "======================================================================\n",
      "\n",
      "Found 1 relevant dataset(s):\n",
      "\n",
      "  📊 global_sst_2023_01.nc\n",
      "     Relevance: 0.608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Research Scenario: Looking for ocean temperature data for climate study\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Natural language queries a researcher might use\n",
    "research_queries = [\n",
    "    \"I need sea surface temperature data\",\n",
    "    \"SST measurements for 2023\",\n",
    "    \"ocean thermal data\",\n",
    "    \"satellite temperature observations\"\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "for query in research_queries:\n",
    "    results = engine.search(query, top_k=1)\n",
    "    if results:\n",
    "        best_match = results[0]\n",
    "        filepath = best_match['filepath']\n",
    "        score = best_match['similarity_score']\n",
    "        \n",
    "        if filepath not in all_results or score > all_results[filepath]:\n",
    "            all_results[filepath] = score\n",
    "\n",
    "print(f\"\\nFound {len(all_results)} relevant dataset(s):\\n\")\n",
    "for filepath, score in sorted(all_results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  📊 {Path(filepath).name}\")\n",
    "    print(f\"     Relevance: {score:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Benchmarks:\n",
      "============================================================\n",
      "Average search time: 0.23 ms\n",
      "Target: <200ms ✓\n",
      "\n",
      "Datasets indexed: 3\n",
      "Ready for production use: ✓\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Measure end-to-end performance\n",
    "print(\"Performance Benchmarks:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Search speed\n",
    "query = \"ocean data\"\n",
    "start = time.time()\n",
    "for _ in range(50):\n",
    "    engine.search(query)\n",
    "avg_search_ms = (time.time() - start) / 50 * 1000\n",
    "\n",
    "print(f\"Average search time: {avg_search_ms:.2f} ms\")\n",
    "print(f\"Target: <200ms \" + (\"✓\" if avg_search_ms < 200 else \"✗\"))\n",
    "\n",
    "print(f\"\\nDatasets indexed: {stats['total_vectors']}\")\n",
    "print(f\"Ready for production use: ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up sample files\n",
    "# shutil.rmtree(sample_dir)\n",
    "# archive_path.unlink()\n",
    "# print(\"✓ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: FAIR Principles Achieved\n",
    "\n",
    "✅ **Findable**\n",
    "- Natural language search works\n",
    "- Metadata extracted and indexed\n",
    "- Fast similarity search\n",
    "\n",
    "✅ **Accessible**\n",
    "- File paths preserved\n",
    "- Metadata easily retrieved\n",
    "- Archive support\n",
    "\n",
    "✅ **Interoperable**\n",
    "- Standard formats (NetCDF, HDF5)\n",
    "- CF conventions supported\n",
    "- Companion docs included\n",
    "\n",
    "✅ **Reusable**\n",
    "- Full metadata preserved\n",
    "- Citations discoverable\n",
    "- Context maintained\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Use with Your Data**: Replace sample files with real datasets\n",
    "2. **Optional LLM Enhancement**: See Notebook 06\n",
    "3. **Command-Line Tools**: Use provided scripts for automation\n",
    "4. **Integration**: Connect with your analysis workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
