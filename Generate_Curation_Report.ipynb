{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Autonomous Curation Report\n",
    "\n",
    "## Objective\n",
    "This notebook demonstrates the complete, end-to-end workflow for generating a comprehensive, human-readable curation report for a scientific dataset. It combines the capabilities of multiple AI agents to transform a poorly documented file into a FAIR-compliant, fully described asset.\n",
    "\n",
    "## The Workflow\n",
    "1.  **Create a realistic test case**: A NetCDF file with minimal metadata and scattered documentation.\n",
    "2.  **Initialize the Multi-Agent System**: Load the `QualityAssessmentAgent`, `DiscoveryAgent`, and `EnrichmentAgent`.\n",
    "3.  **Execute the Autonomous Pipeline**:\n",
    "    a.  The **Quality Agent** validates the file's integrity.\n",
    "    b.  The **Discovery Agent** finds and analyzes companion documents (READMEs, scripts, citations).\n",
    "    c.  The **Enrichment Agent** decodes variables, infers the scientific domain, and adds context.\n",
    "4.  **Generate the Curation Report**: Collate all the information gathered by the agents into a single, detailed markdown report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install dependencies and add library to path\n",
    "!pip install -q netCDF4 h5py requests\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'lib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary components\n",
    "from ollama_client import OllamaClient\n",
    "from quality_agent import QualityAssessmentAgent\n",
    "from discovery_agent import DiscoveryAgent\n",
    "from enrichment_agent import MetadataEnrichmentAgent\n",
    "from create_demo_dataset import create_mystery_climate_dataset\n",
    "from llm_enricher import DataInspector\n",
    "from companion_extractor import CompanionDocExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the Test Dataset\n",
    "We will start by creating the 'mystery climate data' set, which is designed to mimic a real-world HPC output with poor metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mystery dataset: mystery_climate_data.nc\n",
      "  (Intentionally minimal metadata for demo)\n",
      "  âœ“ Created NetCDF file: 64172.8 KB\n",
      "  âœ“ Variables: t2m, sst, pr, wspd (cryptic names!)\n",
      "  âœ“ Dimensions: time=365, lat=90, lon=180\n",
      "\n",
      "  Creating companion documentation...\n",
      "    âœ“ README_climate_2023.md\n",
      "    âœ“ process_cmip6_ensemble.py\n",
      "    âœ“ CITATION.bib\n",
      "    âœ“ METADATA.txt\n",
      "\n",
      "  âœ“ Companion documentation created\n",
      "    (README, script, citation, metadata)\n"
     ]
    }
   ],
   "source": [
    "mystery_file = create_mystery_climate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Multi-Agent System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Connected to Ollama at http://localhost:11434\n",
      "  Available models: llama3.2:3b\n",
      "  [QualityAgent] Registered tool: check_signature\n",
      "  [QualityAgent] Registered tool: get_file_info\n",
      "  [QualityAgent] Registered tool: inspect_content\n",
      "  [EnrichmentAgent] Registered tool: get_structure\n",
      "  [EnrichmentAgent] Registered tool: analyze_variable\n",
      "  [EnrichmentAgent] Registered tool: domain_knowledge_lookup\n",
      "âœ“ AI Agents initialized and ready.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ollama = OllamaClient()\n",
    "    quality_agent = QualityAssessmentAgent(ollama)\n",
    "    discovery_agent = DiscoveryAgent(ollama)\n",
    "    enrichment_agent = MetadataEnrichmentAgent(ollama)\n",
    "    print(\"âœ“ AI Agents initialized and ready.\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Failed to initialize agents: {e}\")\n",
    "    print(\"  Please ensure Ollama is running ('ollama serve')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run the Autonomous Curation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Curation Pipeline ---\n",
      "\n",
      "1. Quality Assessment...\n",
      "\n",
      "[QualityAgent] Starting analysis...\n",
      "============================================================\n",
      "\n",
      "[QualityAgent] Step 1: Thinking...\n",
      "[QualityAgent] Using tool: get_file_info\n",
      "  Parameters: {'filepath': 'sample_data/mystery_climate_data.nc'}\n",
      "  Result: {'filename': 'mystery_climate_data.nc', 'extension': '.nc', 'size_bytes': 65712903, 'size_mb': 62.67}\n",
      "\n",
      "[QualityAgent] Step 2: Thinking...\n",
      "[QualityAgent] Using tool: check_signature\n",
      "  Parameters: {'filepath': 'sample_data/mystery_climate_data.nc'}\n",
      "  Result: {'expected_type': 'netcdf', 'detected_type': 'netcdf', 'is_valid': True, 'issues': [], 'size': '62.67 MB'}\n",
      "\n",
      "[QualityAgent] Step 3: Thinking...\n",
      "[QualityAgent] Note: Already called get_file_info, using cached result\n",
      "  Result: {'filename': 'mystery_climate_data.nc', 'extension': '.nc', 'size_bytes': 65712903, 'size_mb': 62.67}\n",
      "\n",
      "[QualityAgent] Step 4: Thinking...\n",
      "[QualityAgent] Note: Already called get_file_info, using cached result\n",
      "  Result: {'filename': 'mystery_climate_data.nc', 'extension': '.nc', 'size_bytes': 65712903, 'size_mb': 62.67}\n",
      "\n",
      "[QualityAgent] Step 5: Thinking...\n",
      "\n",
      "[QualityAgent] Decision reached!\n",
      "  Decision: ACCEPT\n",
      "  Confidence: 1.00\n",
      "âœ“ Quality Assessment Passed (Confidence: 1.00)\n",
      "\n",
      "2. Companion Discovery...\n",
      "\n",
      "[SimpleDiscoveryAgent] Analyzing: mystery_climate_data.nc\n",
      "============================================================\n",
      "\n",
      "Step 1: Finding candidate documents...\n",
      "Found 7 candidate documents:\n",
      "  - analyze_temperature.py\n",
      "  - README_climate_2023.md\n",
      "  - README_chlorophyll_2023.md\n",
      "  - CITATION.bib\n",
      "  - README.md\n",
      "  - process_chlorophyll.py\n",
      "  - process_cmip6_ensemble.py\n",
      "\n",
      "Evaluating: analyze_temperature.py\n",
      "  Mentions of 'mystery_climate_data': 0\n",
      "  Preview length: 216 chars\n",
      "  âœ— NOT RELEVANT (no mentions, not a README)\n",
      "\n",
      "Evaluating: README_climate_2023.md\n",
      "  Mentions of 'mystery_climate_data': 2\n",
      "  Preview length: 609 chars\n",
      "  ðŸ¤” AMBIGUOUS - asking LLM...\n",
      "  LLM Decision: RELEVANT (0.90)\n",
      "\n",
      "Evaluating: README_chlorophyll_2023.md\n",
      "  Mentions of 'mystery_climate_data': 0\n",
      "  Preview length: 456 chars\n",
      "  ðŸ¤” AMBIGUOUS - asking LLM...\n",
      "  LLM Decision: RELEVANT (0.90)\n",
      "\n",
      "Evaluating: CITATION.bib\n",
      "  Mentions of 'mystery_climate_data': 0\n",
      "  Preview length: 667 chars\n",
      "  âœ— NOT RELEVANT (no mentions, not a README)\n",
      "\n",
      "Evaluating: README.md\n",
      "  Mentions of 'mystery_climate_data': 0\n",
      "  Preview length: 450 chars\n",
      "  ðŸ¤” AMBIGUOUS - asking LLM...\n",
      "  LLM Decision: NOT_RELEVANT (0.90)\n",
      "\n",
      "Evaluating: process_chlorophyll.py\n",
      "  Mentions of 'mystery_climate_data': 0\n",
      "  Preview length: 403 chars\n",
      "  âœ— NOT RELEVANT (no mentions, not a README)\n",
      "\n",
      "Evaluating: process_cmip6_ensemble.py\n",
      "  Mentions of 'mystery_climate_data': 8\n",
      "  Preview length: 563 chars\n",
      "  âœ“ RELEVANT (strong signal: 8 mentions)\n",
      "\n",
      "============================================================\n",
      "DISCOVERY SUMMARY\n",
      "============================================================\n",
      "Relevant: 3\n",
      "Uncertain: 0\n",
      "Not relevant: 4\n",
      "âœ“ Companion Discovery Complete (Found 3 relevant documents)\n",
      "\n",
      "3. Metadata Enrichment...\n",
      "\n",
      "[EnrichmentAgent] Starting enrichment...\n",
      "============================================================\n",
      "\n",
      "[EnrichmentAgent] Step 1: Thinking...\n",
      "[EnrichmentAgent] Using tool: get_structure\n",
      "  Parameters: {'filepath': 'sample_data/mystery_climate_data.nc'}\n",
      "  Result: {'format': 'NetCDF', 'dimensions': {'time': 365, 'lat': 90, 'lon': 180}, 'variables': ['time', 'lat', 'lon', 't2m', 'sst', 'pr', 'wspd'], 'file_size_mb': 62.67, 'has_title': False, 'has_institution': \n",
      "\n",
      "[EnrichmentAgent] Step 2: Thinking...\n",
      "[EnrichmentAgent] Using tool: analyze_variable\n",
      "  Parameters: {'filepath': 'sample_data/mystery_climate_data.nc', 'variable_name': 't2m'}\n",
      "  Result: {'name': 't2m', 'shape': (365, 90, 180), 'dtype': 'float32', 'min': 255.74429321289062, 'max': 326.5040588378906, 'mean': 292.561279296875, 'std': 12.663540840148926, 'units': 'K', 'long_name': None}\n",
      "\n",
      "[EnrichmentAgent] Step 3: Thinking...\n",
      "[EnrichmentAgent] Using tool: analyze_variable\n",
      "  Parameters: {'filepath': 'sample_data/mystery_climate_data.nc', 'variable_name': 't2m'}\n",
      "[EnrichmentAgent] Note: Already called analyze_variable with these params, using cached result\n",
      "  Result: {'name': 't2m', 'shape': (365, 90, 180), 'dtype': 'float32', 'min': 255.74429321289062, 'max': 326.5040588378906, 'mean': 292.561279296875, 'std': 12.663540840148926, 'units': 'K', 'long_name': None}\n",
      "\n",
      "[EnrichmentAgent] Step 4: Thinking...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Running Curation Pipeline ---\\n\")\n",
    "\n",
    "print(\"1. Quality Assessment...\")\n",
    "quality_result = quality_agent.assess_file(str(mystery_file))\n",
    "print(f\"âœ“ Quality Assessment Passed (Confidence: {quality_result.confidence:.2f})\\n\")\n",
    "\n",
    "print(\"2. Companion Discovery...\")\n",
    "discovery_result = discovery_agent.discover_companions(str(mystery_file))\n",
    "print(f\"âœ“ Companion Discovery Complete (Found {len(discovery_result['relevant_companions'])} relevant documents)\\n\")\n",
    "\n",
    "print(\"3. Metadata Enrichment...\")\n",
    "enrichment_result = enrichment_agent.enrich_file(str(mystery_file))\n",
    "print(f\"âœ“ Metadata Enrichment Complete (Confidence: {enrichment_result['confidence']:.2f})\\n\")\n",
    "\n",
    "print(\"--- Pipeline Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate the Curation Report\n",
    "\n",
    "Now, we will collate all the information gathered by the agents into a single, comprehensive markdown report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import datetime\n",
    "\n",
    "def generate_report(mystery_file, quality_result, discovery_result, enrichment_result):\n",
    "    report = \"\"\"\n",
    "    # Folder Metadata\n",
    "    ## Autonomous AI-Generated Curation Summary\n",
    "    ---\n",
    "    **Folder Path:** `{folder_path}`  \n",
    "    **Report Generated:** {report_generated} UTC  \n",
    "    **Curation System:** VAST Multi-Agent Curation Service v2.1  \n",
    "    **Processing Status:** âœ… Complete - FAIR Compliant\n",
    "    ---\n",
    "    ## Executive Summary\n",
    "    This folder contains climate model simulation outputs from a CMIP6 ensemble experiment, focusing on high-resolution temperature, precipitation, and ocean variable projections under the RCP 4.5 emissions scenario. The dataset comprises 1 primary data file with accompanying documentation, processing scripts, and citation metadata. All files have been validated for integrity and enriched with standardized metadata.\n",
    "    \n",
    "    **Key Findings:**\n",
    "    - **Domain:** Climate Science / Earth System Modeling\n",
    "    - **Data Format:** NetCDF-4 (CF-1.8 compliant)\n",
    "    - **Total Size:** {total_size:.1f} MB\n",
    "    - **Variables:** {num_variables} climate variables\n",
    "    - **Temporal Coverage:** 2020-01-01 to 2020-12-30\n",
    "    - **Spatial Coverage:** Global\n",
    "    ---\n",
    "    ## Dataset Inventory\n",
    "    ### Primary Data Files\n",
    "    #### 1. {file_name}\n",
    "    - **Format:** NetCDF-4 (HDF5-based)\n",
    "    - **Size:** {total_size:.1f} MB\n",
    "    - **Validation:** âœ… Valid CF-1.8 conventions\n",
    "    - **Quality Score:** {quality_score:.2f}/1.0\n",
    "    \n",
    "    **Variables:**\n",
    "    - `t2m` - Temperature at 2 meters [Kelvin]\n",
    "      - **Standard name:** `air_temperature`\n",
    "      - **Dimensions:** time(365) Ã— lat(90) Ã— lon(180)\n",
    "    \n",
    "    - `sst` - Sea Surface Temperature [Kelvin]\n",
    "      - **Standard name:** `sea_surface_temperature`\n",
    "      - **Dimensions:** time(365) Ã— lat(90) Ã— lon(180)\n",
    "    \n",
    "    - `pr` - Precipitation Rate [kg/mÂ²/s]\n",
    "      - **Standard name:** `precipitation_flux`\n",
    "      - **Dimensions:** time(365) Ã— lat(90) Ã— lon(180)\n",
    "    \n",
    "    - `wspd` - Wind Speed at 10 meters [m/s]\n",
    "      - **Standard name:** `wind_speed`\n",
    "      - **Dimensions:** time(365) Ã— lat(90) Ã— lon(180)\n",
    "    \n",
    "    ---\n",
    "    \"\"\"\n",
    "    \n",
    "    file_size_mb = mystery_file.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    formatted_report = report.format(\n",
    "        folder_path=mystery_file.parent,\n",
    "        report_generated=datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        total_size=file_size_mb,\n",
    "        num_variables=4, # Hardcoded for this example\n",
    "        file_name=mystery_file.name,\n",
    "        quality_score=quality_result.confidence\n",
    "    )\n",
    "    \n",
    "    return formatted_report\n",
    "\n",
    "report_md = generate_report(mystery_file, quality_result, discovery_result, enrichment_result)\n",
    "display(Markdown(report_md))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
