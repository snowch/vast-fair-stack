{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Autonomous Curation Report\n",
    "\n",
    "## Objective\n",
    "This notebook demonstrates the complete, end-to-end workflow for generating a comprehensive, human-readable curation report for a scientific dataset. It combines the capabilities of multiple AI agents to transform a poorly documented file into a FAIR-compliant, fully described asset.\n",
    "\n",
    "## The Workflow\n",
    "1.  **Create a realistic test case**: A NetCDF file with minimal metadata and scattered documentation.\n",
    "2.  **Initialize the Multi-Agent System**: Load the `QualityAssessmentAgent`, `DiscoveryAgent`, and `EnrichmentAgent`.\n",
    "3.  **Execute the Autonomous Pipeline**:\n",
    "    a.  The **Quality Agent** validates the file's integrity.\n",
    "    b.  The **Discovery Agent** finds and analyzes companion documents (READMEs, scripts, citations).\n",
    "    c.  The **Enrichment Agent** decodes variables, infers the scientific domain, and adds context.\n",
    "4.  **Generate the Curation Report**: Collate all the information gathered by the agents into a single, detailed markdown report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install dependencies and add library to path\n",
    "!pip install -q netCDF4 h5py requests\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'lib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary components\n",
    "from ollama_client import OllamaClient\n",
    "from quality_agent import QualityAssessmentAgent\n",
    "from discovery_agent import DiscoveryAgent\n",
    "from enrichment_agent import MetadataEnrichmentAgent\n",
    "from create_demo_dataset import create_mystery_climate_dataset\n",
    "from llm_enricher import DataInspector\n",
    "from companion_extractor import CompanionDocExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the Test Dataset\n",
    "We will start by creating the 'mystery climate data' set, which is designed to mimic a real-world HPC output with poor metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mystery dataset: mystery_climate_data.nc\n",
      "  (Intentionally minimal metadata for demo)\n",
      "  âœ“ Created NetCDF file: 64173.1 KB\n",
      "  âœ“ Variables: t2m, sst, pr, wspd (cryptic names!)\n",
      "  âœ“ Dimensions: time=365, lat=90, lon=180\n",
      "\n",
      "  Creating companion documentation...\n",
      "    âœ“ README_climate_2023.md\n",
      "    âœ“ process_cmip6_ensemble.py\n",
      "    âœ“ CITATION.bib\n",
      "    âœ“ METADATA.txt\n",
      "\n",
      "  âœ“ Companion documentation created\n",
      "    (README, script, citation, metadata)\n"
     ]
    }
   ],
   "source": [
    "mystery_file = create_mystery_climate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Multi-Agent System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Connected to Ollama at http://localhost:11434\n",
      "  Available models: llama3.2:3b\n",
      "  [QualityAgent] Registered tool: check_signature\n",
      "  [QualityAgent] Registered tool: get_file_info\n",
      "  [QualityAgent] Registered tool: inspect_content\n",
      "  [EnrichmentAgent] Registered tool: get_structure\n",
      "  [EnrichmentAgent] Registered tool: analyze_variable\n",
      "  [EnrichmentAgent] Registered tool: domain_knowledge_lookup\n",
      "âœ“ AI Agents initialized and ready.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ollama = OllamaClient()\n",
    "    quality_agent = QualityAssessmentAgent(ollama)\n",
    "    discovery_agent = DiscoveryAgent(ollama)\n",
    "    enrichment_agent = MetadataEnrichmentAgent(ollama)\n",
    "    print(\"âœ“ AI Agents initialized and ready.\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Failed to initialize agents: {e}\")\n",
    "    print(\"  Please ensure Ollama is running ('ollama serve')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run the Autonomous Curation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Curation Pipeline ---\n",
      "\n",
      "1. Quality Assessment...\n",
      "\n",
      "[QualityAgent] Starting analysis...\n",
      "============================================================\n",
      "\n",
      "[QualityAgent] Step 1: Thinking...\n",
      "[QualityAgent] Using tool: get_file_info\n",
      "  Parameters: {'filepath': 'generated/sample_data/mystery_climate_data.nc'}\n",
      "  Result: {'filename': 'mystery_climate_data.nc', 'extension': '.nc', 'size_bytes': 65713293, 'size_mb': 62.67}\n",
      "\n",
      "[QualityAgent] Step 2: Thinking...\n",
      "[QualityAgent] Using tool: check_signature\n",
      "  Parameters: {'filepath': 'generated/sample_data/mystery_climate_data.nc'}\n",
      "  Result: {'expected_type': 'netcdf', 'detected_type': 'netcdf', 'is_valid': True, 'issues': [], 'size': '62.67 MB'}\n",
      "\n",
      "[QualityAgent] Step 3: Thinking...\n",
      "\n",
      "[QualityAgent] Decision reached!\n",
      "  Decision: MANUAL_REVIEW\n",
      "  Confidence: 0.80\n",
      "âœ“ Quality Assessment Passed (Confidence: 0.80)\n",
      "\n",
      "2. Companion Discovery...\n",
      "\n",
      "[SimpleDiscoveryAgent] Analyzing: mystery_climate_data.nc\n",
      "============================================================\n",
      "\n",
      "Step 1: Finding candidate documents...\n",
      "Found 7 candidate documents:\n",
      "  - CITATION.bib\n",
      "  - analyze_temperature.py\n",
      "  - process_cmip6_ensemble.py\n",
      "  - README_climate_2023.md\n",
      "  - README_chlorophyll_2023.md\n",
      "  - process_chlorophyll.py\n",
      "  - README.md\n",
      "\n",
      "Evaluating: CITATION.bib\n",
      "  Mentions of 'mystery_climate_data': 0\n",
      "  Preview length: 667 chars\n",
      "  âœ— NOT RELEVANT (no mentions, not a README)\n",
      "\n",
      "Evaluating: analyze_temperature.py\n",
      "  Mentions of 'mystery_climate_data': 0\n",
      "  Preview length: 216 chars\n",
      "  âœ— NOT RELEVANT (no mentions, not a README)\n",
      "\n",
      "Evaluating: process_cmip6_ensemble.py\n",
      "  Mentions of 'mystery_climate_data': 8\n",
      "  Preview length: 563 chars\n",
      "  âœ“ RELEVANT (strong signal: 8 mentions)\n",
      "\n",
      "Evaluating: README_climate_2023.md\n",
      "  Mentions of 'mystery_climate_data': 2\n",
      "  Preview length: 609 chars\n",
      "  ðŸ¤” AMBIGUOUS - asking LLM...\n",
      "  LLM Decision: RELEVANT (0.90)\n",
      "\n",
      "Evaluating: README_chlorophyll_2023.md\n",
      "  Mentions of 'mystery_climate_data': 0\n",
      "  Preview length: 456 chars\n",
      "  ðŸ¤” AMBIGUOUS - asking LLM...\n",
      "  LLM Decision: RELEVANT (0.80)\n",
      "\n",
      "Evaluating: process_chlorophyll.py\n",
      "  Mentions of 'mystery_climate_data': 0\n",
      "  Preview length: 403 chars\n",
      "  âœ— NOT RELEVANT (no mentions, not a README)\n",
      "\n",
      "Evaluating: README.md\n",
      "  Mentions of 'mystery_climate_data': 0\n",
      "  Preview length: 450 chars\n",
      "  ðŸ¤” AMBIGUOUS - asking LLM...\n",
      "  LLM Decision: NOT_RELEVANT (0.80)\n",
      "\n",
      "============================================================\n",
      "DISCOVERY SUMMARY\n",
      "============================================================\n",
      "Relevant: 3\n",
      "Uncertain: 0\n",
      "Not relevant: 4\n",
      "âœ“ Companion Discovery Complete (Found 3 relevant documents)\n",
      "\n",
      "3. Metadata Enrichment...\n",
      "\n",
      "[EnrichmentAgent] Starting enrichment...\n",
      "============================================================\n",
      "\n",
      "[EnrichmentAgent] Step 1: Thinking...\n",
      "[EnrichmentAgent] Using tool: get_structure\n",
      "  Parameters: {'filepath': 'generated/sample_data/mystery_climate_data.nc'}\n",
      "  Result: {'format': 'NetCDF', 'dimensions': {'time': 365, 'lat': 90, 'lon': 180}, 'variables': ['time', 'lat', 'lon', 't2m', 'sst', 'pr', 'wspd'], 'file_size_mb': 62.67, 'has_title': False, 'has_institution': \n",
      "\n",
      "[EnrichmentAgent] Step 2: Thinking...\n",
      "[EnrichmentAgent] Using tool: domain_knowledge_lookup\n",
      "  Parameters: {'variables': ['t2m', 'sst', 'pr', 'wspd']}\n",
      "  [Ignored] Invalid parameter 'variables' for domain_knowledge_lookup\n",
      "  Result: {'error': 'Missing required parameter: term'}\n",
      "\n",
      "[EnrichmentAgent] Step 3: Thinking...\n",
      "[EnrichmentAgent] Using tool: get_structure\n",
      "  Parameters: {'filepath': 'generated/sample_data/mystery_climate_data.nc'}\n",
      "[EnrichmentAgent] Note: Already called get_structure with these params, using cached result\n",
      "  Result: {'format': 'NetCDF', 'dimensions': {'time': 365, 'lat': 90, 'lon': 180}, 'variables': ['time', 'lat', 'lon', 't2m', 'sst', 'pr', 'wspd'], 'file_size_mb': 62.67, 'has_title': False, 'has_institution': \n",
      "\n",
      "[EnrichmentAgent] Step 4: Thinking...\n",
      "[EnrichmentAgent] Using tool: domain_knowledge_lookup\n",
      "  Parameters: {'variables': ['t2m', 'sst', 'pr', 'wspd']}\n",
      "  [Ignored] Invalid parameter 'variables' for domain_knowledge_lookup\n",
      "  Result: {'error': 'Missing required parameter: term'}\n",
      "\n",
      "[EnrichmentAgent] Step 5: Thinking...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Running Curation Pipeline ---\\n\")\n",
    "\n",
    "print(\"1. Quality Assessment...\")\n",
    "quality_result = quality_agent.assess_file(str(mystery_file))\n",
    "print(f\"âœ“ Quality Assessment Passed (Confidence: {quality_result.confidence:.2f})\\n\")\n",
    "\n",
    "print(\"2. Companion Discovery...\")\n",
    "discovery_result = discovery_agent.discover_companions(str(mystery_file))\n",
    "print(f\"âœ“ Companion Discovery Complete (Found {len(discovery_result['relevant_companions'])} relevant documents)\\n\")\n",
    "\n",
    "print(\"3. Metadata Enrichment...\")\n",
    "enrichment_result = enrichment_agent.enrich_file(str(mystery_file))\n",
    "print(f\"âœ“ Metadata Enrichment Complete (Confidence: {enrichment_result['confidence']:.2f})\\n\")\n",
    "\n",
    "print(\"--- Pipeline Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate the Curation Report\n",
    "\n",
    "Now, we will collate all the information gathered by the agents into a single, comprehensive markdown report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from report_generator import LLMReportGenerator\n",
    "\n",
    "# Initialize the report generator with the ollama client\n",
    "llm_report_generator = LLMReportGenerator(ollama)\n",
    "\n",
    "# Generate the report\n",
    "report_md = llm_report_generator.generate_report(\n",
    "    mystery_file, \n",
    "    quality_result, \n",
    "    discovery_result, \n",
    "    enrichment_result\n",
    ")\n",
    "\n",
    "# Display the report\n",
    "display(Markdown(report_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
