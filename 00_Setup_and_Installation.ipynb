{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR Scientific Data Discovery System - Setup\n",
    "\n",
    "This notebook guides you through installing and setting up the system.\n",
    "\n",
    "## What is FAIR?\n",
    "\n",
    "**FAIR** stands for:\n",
    "- **F**indable: Datasets can be discovered through rich metadata.\n",
    "- **A**ccessible: Clear paths to access data.\n",
    "- **I**nteroperable: Works with standard formats.\n",
    "- **R**eusable: Well-documented for future use.\n",
    "\n",
    "## System Overview\n",
    "\n",
    "This system makes scientific datasets (NetCDF, HDF5, GRIB) more discoverable by:\n",
    "- âœ… Extracting rich metadata automatically.\n",
    "- âœ… Handling even poorly documented files.\n",
    "- âœ… Discovering and linking companion documentation (READMEs, scripts, citations).\n",
    "- âœ… Supporting LLM enrichment for enhanced context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, let's install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies\n",
    "!pip install -q netCDF4 h5py numpy tqdm requests pygrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ netCDF4 installed\n",
      "âœ“ h5py installed\n",
      "âœ“ pygrib installed\n",
      "âœ“ requests installed\n"
     ]
    }
   ],
   "source": [
    "# Test imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import netCDF4\n",
    "    print(\"âœ“ netCDF4 installed\")\n",
    "except ImportError:\n",
    "    print(\"âœ— netCDF4 not installed\")\n",
    "\n",
    "try:\n",
    "    import h5py\n",
    "    print(\"âœ“ h5py installed\")\n",
    "except ImportError:\n",
    "    print(\"âœ— h5py not installed\")\n",
    "\n",
    "try:\n",
    "    import pygrib\n",
    "    print(\"âœ“ pygrib installed\")\n",
    "except ImportError:\n",
    "    print(\"âœ— pygrib not installed\")\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    print(\"âœ“ requests installed\")\n",
    "except ImportError:\n",
    "    print(\"âœ— requests not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test System Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ config module loaded\n",
      "âœ“ file_validator module loaded\n",
      "âœ“ metadata_extractors module loaded\n",
      "âœ“ companion_finder module loaded\n",
      "\n",
      "âœ“ All core modules loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path to import our modules\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    import config\n",
    "    print(\"âœ“ config module loaded\")\n",
    "    \n",
    "    from file_validator import FileValidator\n",
    "    print(\"âœ“ file_validator module loaded\")\n",
    "    \n",
    "    from metadata_extractors import MetadataExtractor\n",
    "    print(\"âœ“ metadata_extractors module loaded\")\n",
    "    \n",
    "    from companion_finder import CompanionDocFinder\n",
    "    print(\"âœ“ companion_finder module loaded\")\n",
    "    \n",
    "    print(\"\\nâœ“ All core modules loaded successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âœ— Error importing modules: {e}\")\n",
    "    print(\"Make sure all .py files are in the correct directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Sample Data\n",
    "\n",
    "Let's create a simple NetCDF file for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Sample file created: generated/sample_data/ocean_temperature.nc\n",
      "  Size: 34266 bytes\n"
     ]
    }
   ],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "\n",
    "# Create sample directory in generated/\n",
    "sample_dir = Path(\"generated/sample_data\")\n",
    "sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a simple NetCDF file\n",
    "filepath = sample_dir / \"ocean_temperature.nc\"\n",
    "\n",
    "with netCDF4.Dataset(filepath, 'w') as ds:\n",
    "    ds.title = \"Sample Ocean Temperature Data\"\n",
    "    ds.institution = \"Demo University\"\n",
    "    ds.source = \"Simulated data for testing\"\n",
    "    ds.Conventions = \"CF-1.8\"\n",
    "    \n",
    "    ds.createDimension('time', 10)\n",
    "    ds.createDimension('lat', 20)\n",
    "    ds.createDimension('lon', 30)\n",
    "    \n",
    "    time = ds.createVariable('time', 'f8', ('time',))\n",
    "    time.units = 'days since 2020-01-01'\n",
    "    time[:] = np.arange(10)\n",
    "    \n",
    "    lat = ds.createVariable('lat', 'f4', ('lat',))\n",
    "    lat.units = 'degrees_north'\n",
    "    lat[:] = np.linspace(-90, 90, 20)\n",
    "    \n",
    "    lon = ds.createVariable('lon', 'f4', ('lon',))\n",
    "    lon.units = 'degrees_east'\n",
    "    lon[:] = np.linspace(-180, 180, 30)\n",
    "    \n",
    "    temp = ds.createVariable('sea_surface_temperature', 'f4', ('time', 'lat', 'lon'))\n",
    "    temp.units = 'celsius'\n",
    "    temp.long_name = 'Sea Surface Temperature'\n",
    "    temp.standard_name = 'sea_surface_temperature'\n",
    "    temp[:] = np.random.randn(10, 20, 30) * 5 + 15\n",
    "\n",
    "print(f\"âœ“ Sample file created: {filepath}\")\n",
    "print(f\"  Size: {filepath.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Quick Test\n",
    "\n",
    "Let's do a quick test of the metadata extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing metadata extractor...\n",
      "Extracting metadata from sample file...\n",
      "\n",
      "--- Extracted Metadata ---\n",
      "Title: Sample Ocean Temperature Data\n",
      "Institution: Demo University\n",
      "Format: NetCDF\n",
      "Variables: time, lat, lon, sea_surface_temperature\n",
      "--------------------------\n",
      "\n",
      "ðŸŽ‰ System test complete!\n"
     ]
    }
   ],
   "source": [
    "from metadata_extractors import MetadataExtractor\n",
    "\n",
    "# Initialize extractor\n",
    "print(\"Initializing metadata extractor...\")\n",
    "extractor = MetadataExtractor()\n",
    "\n",
    "# Extract metadata from the sample file\n",
    "print(\"Extracting metadata from sample file...\")\n",
    "metadata = extractor.extract(filepath)\n",
    "\n",
    "if 'error' not in metadata:\n",
    "    print(\"\\n--- Extracted Metadata ---\")\n",
    "    print(f\"Title: {metadata.get('title', 'N/A')}\")\n",
    "    print(f\"Institution: {metadata.get('institution', 'N/A')}\")\n",
    "    print(f\"Format: {metadata.get('format', 'N/A')}\")\n",
    "    print(f\"Variables: {', '.join(metadata.get('variables', {}).keys())}\")\n",
    "    print(\"--------------------------\")\n",
    "else:\n",
    "    print(f\"âœ— Error during extraction: {metadata['error']}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ System test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that the system is set up, you can:\n",
    "\n",
    "1. **Explore Components**: Check out the other notebooks to learn about each component.\n",
    "2. **Process Your Data**: Use the `fair_index.py` script to process your own scientific datasets.\n",
    "3. **Try Full Workflow**: See notebook 99 for end-to-end examples.\n",
    "\n",
    "## Install Ollama (for LLM Enrichment)\n",
    "\n",
    "To enable LLM-based metadata enrichment:\n",
    "\n",
    "```bash\n",
    "# Install Ollama\n",
    "curl -fsSL [https://ollama.com/install.sh](https://ollama.com/install.sh) | sh\n",
    "\n",
    "# Pull a model\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "\n",
    "See notebook 06 for LLM enrichment examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
