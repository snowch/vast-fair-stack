{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR Scientific Data Discovery System - Setup\n",
    "\n",
    "This notebook guides you through installing and setting up the system.\n",
    "\n",
    "## What is FAIR?\n",
    "\n",
    "**FAIR** stands for:\n",
    "- **F**indable: Datasets can be discovered through search\n",
    "- **A**ccessible: Clear paths to access data\n",
    "- **I**nteroperable: Works with standard formats\n",
    "- **R**eusable: Well-documented for future use\n",
    "\n",
    "## System Overview\n",
    "\n",
    "This system makes scientific datasets (NetCDF, HDF5, GRIB) searchable using:\n",
    "- âœ… Local semantic search (no API keys needed)\n",
    "- âœ… Works offline\n",
    "- âœ… Handles minimal metadata\n",
    "- âœ… Discovers companion documentation\n",
    "- âœ… Optional LLM enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, let's install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q netCDF4 h5py sentence-transformers faiss-cpu numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install pygrib for GRIB support (may require system dependencies)\n",
    "# !pip install -q pygrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install ollama-python for LLM enrichment\n",
    "# !pip install -q ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Embedding Model\n",
    "\n",
    "The system uses a local sentence-transformer model for generating embeddings.\n",
    "This will download ~80MB on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download and cache the model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"Downloading model: {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"âœ“ Model loaded! Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import netCDF4\n",
    "    print(\"âœ“ netCDF4 installed\")\n",
    "except ImportError:\n",
    "    print(\"âœ— netCDF4 not installed\")\n",
    "\n",
    "try:\n",
    "    import h5py\n",
    "    print(\"âœ“ h5py installed\")\n",
    "except ImportError:\n",
    "    print(\"âœ— h5py not installed\")\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    print(\"âœ“ faiss installed\")\n",
    "except ImportError:\n",
    "    print(\"âœ— faiss not installed\")\n",
    "\n",
    "try:\n",
    "    import sentence_transformers\n",
    "    print(\"âœ“ sentence-transformers installed\")\n",
    "except ImportError:\n",
    "    print(\"âœ— sentence-transformers not installed\")\n",
    "\n",
    "try:\n",
    "    import pygrib\n",
    "    print(\"âœ“ pygrib installed\")\n",
    "except ImportError:\n",
    "    print(\"âš  pygrib not installed (optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test System Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import our modules\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    import config\n",
    "    print(\"âœ“ config module loaded\")\n",
    "    \n",
    "    from file_validator import FileValidator\n",
    "    print(\"âœ“ file_validator module loaded\")\n",
    "    \n",
    "    from metadata_extractors import MetadataExtractor\n",
    "    print(\"âœ“ metadata_extractors module loaded\")\n",
    "    \n",
    "    from embedding_generator import EmbeddingGenerator\n",
    "    print(\"âœ“ embedding_generator module loaded\")\n",
    "    \n",
    "    from vector_index import VectorIndex\n",
    "    print(\"âœ“ vector_index module loaded\")\n",
    "    \n",
    "    from search_engine import FAIRSearchEngine\n",
    "    print(\"âœ“ search_engine module loaded\")\n",
    "    \n",
    "    print(\"\\nâœ“ All modules loaded successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âœ— Error importing modules: {e}\")\n",
    "    print(\"Make sure all .py files are in the parent directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Sample Data (Optional)\n",
    "\n",
    "Let's create a simple NetCDF file for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "\n",
    "# Create sample directory\n",
    "sample_dir = Path(\"sample_data\")\n",
    "sample_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a simple NetCDF file\n",
    "filepath = sample_dir / \"ocean_temperature.nc\"\n",
    "\n",
    "with netCDF4.Dataset(filepath, 'w') as ds:\n",
    "    # Global attributes\n",
    "    ds.title = \"Sample Ocean Temperature Data\"\n",
    "    ds.institution = \"Demo University\"\n",
    "    ds.source = \"Simulated data for testing\"\n",
    "    ds.Conventions = \"CF-1.8\"\n",
    "    \n",
    "    # Create dimensions\n",
    "    ds.createDimension('time', 10)\n",
    "    ds.createDimension('lat', 20)\n",
    "    ds.createDimension('lon', 30)\n",
    "    \n",
    "    # Create variables\n",
    "    time = ds.createVariable('time', 'f8', ('time',))\n",
    "    time.units = 'days since 2020-01-01'\n",
    "    time[:] = np.arange(10)\n",
    "    \n",
    "    lat = ds.createVariable('lat', 'f4', ('lat',))\n",
    "    lat.units = 'degrees_north'\n",
    "    lat[:] = np.linspace(-90, 90, 20)\n",
    "    \n",
    "    lon = ds.createVariable('lon', 'f4', ('lon',))\n",
    "    lon.units = 'degrees_east'\n",
    "    lon[:] = np.linspace(-180, 180, 30)\n",
    "    \n",
    "    temp = ds.createVariable('sea_surface_temperature', 'f4', ('time', 'lat', 'lon'))\n",
    "    temp.units = 'celsius'\n",
    "    temp.long_name = 'Sea Surface Temperature'\n",
    "    temp.standard_name = 'sea_surface_temperature'\n",
    "    temp[:] = np.random.randn(10, 20, 30) * 5 + 15\n",
    "\n",
    "print(f\"âœ“ Sample file created: {filepath}\")\n",
    "print(f\"  Size: {filepath.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Quick Test\n",
    "\n",
    "Let's do a quick test of the complete system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_engine import FAIRSearchEngine\n",
    "\n",
    "# Initialize search engine\n",
    "print(\"Initializing search engine...\")\n",
    "engine = FAIRSearchEngine(load_existing=False)\n",
    "\n",
    "# Index the sample file\n",
    "print(\"\\nIndexing sample file...\")\n",
    "result = engine.index_file(filepath)\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "# Search for it\n",
    "print(\"\\nSearching for 'ocean temperature'...\")\n",
    "results = engine.search(\"ocean temperature\", top_k=1)\n",
    "\n",
    "if results:\n",
    "    print(f\"âœ“ Found {len(results)} result(s)\")\n",
    "    print(f\"  File: {results[0]['filepath']}\")\n",
    "    print(f\"  Score: {results[0]['similarity_score']:.3f}\")\n",
    "else:\n",
    "    print(\"âœ— No results found\")\n",
    "\n",
    "# Save index\n",
    "print(\"\\nSaving index...\")\n",
    "engine.save()\n",
    "print(\"âœ“ Index saved\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ System test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that the system is set up, you can:\n",
    "\n",
    "1. **Explore Components**: Check out notebooks 01-06 to learn about each component\n",
    "2. **Index Your Data**: Use notebook 05 to index your scientific datasets\n",
    "3. **Try Full Workflow**: See notebook 99 for end-to-end examples\n",
    "\n",
    "## Optional: Install Ollama (for LLM Enrichment)\n",
    "\n",
    "To enable LLM-based metadata enrichment:\n",
    "\n",
    "```bash\n",
    "# Install Ollama\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Pull a model\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "\n",
    "See notebook 06 for LLM enrichment examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
