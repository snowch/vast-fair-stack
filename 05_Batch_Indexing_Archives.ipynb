{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Indexing and Archive Processing\n",
    "\n",
    "This notebook demonstrates production-ready workflows:\n",
    "- **Batch indexing** directories with many files\n",
    "- **Archive processing** (.zip, .tar.gz extraction)\n",
    "- **Performance optimization** for large datasets\n",
    "- **Error handling** and validation\n",
    "- **Incremental updates** to existing indexes\n",
    "\n",
    "## Why Batch Processing?\n",
    "\n",
    "Real scientific datasets:\n",
    "- Often contain 100s-1000s of files\n",
    "- Frequently distributed as archives (Zenodo, Figshare)\n",
    "- Need robust error handling\n",
    "- Require progress tracking\n",
    "\n",
    "## Workflow\n",
    "\n",
    "```\n",
    "Data Directory â†’ Validation â†’ Extraction â†’ Companion Discovery â†’ Indexing â†’ Search\n",
    "     â†“\n",
    "Archives (.zip) â†’ Extract â†’ Find Data Files â†’ Process Each â†’ Add to Index\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from search_engine import FAIRSearchEngine\n",
    "from archive_handler import ArchiveHandler, ArchiveAwareIndexer\n",
    "from file_validator import FileValidator\n",
    "from metadata_extractors import MetadataExtractor\n",
    "from companion_finder import CompanionDocFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Create Test Dataset with Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test dataset...\n",
      "  âœ“ ocean_temp_jan_2023.nc\n",
      "  âœ“ ocean_temp_feb_2023.nc\n",
      "  âœ“ ocean_temp_mar_2023.nc\n",
      "  âœ“ wind_speed_jan_2023.nc\n",
      "  âœ“ wind_speed_feb_2023.nc\n",
      "\n",
      "âœ“ Created 5 files in batch_test_data/\n"
     ]
    }
   ],
   "source": [
    "import netCDF4\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Create test directory\n",
    "test_dir = Path(\"batch_test_data\")\n",
    "test_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create multiple NetCDF files\n",
    "datasets = [\n",
    "    {'name': 'ocean_temp_jan_2023.nc', 'var': 'temperature', 'value': 15},\n",
    "    {'name': 'ocean_temp_feb_2023.nc', 'var': 'temperature', 'value': 16},\n",
    "    {'name': 'ocean_temp_mar_2023.nc', 'var': 'temperature', 'value': 17},\n",
    "    {'name': 'wind_speed_jan_2023.nc', 'var': 'wind', 'value': 10},\n",
    "    {'name': 'wind_speed_feb_2023.nc', 'var': 'wind', 'value': 12},\n",
    "]\n",
    "\n",
    "print(\"Creating test dataset...\")\n",
    "for ds_info in datasets:\n",
    "    filepath = test_dir / ds_info['name']\n",
    "    \n",
    "    with netCDF4.Dataset(filepath, 'w') as ds:\n",
    "        ds.title = f\"Test {ds_info['var']} data\"\n",
    "        ds.institution = \"Demo Lab\"\n",
    "        \n",
    "        ds.createDimension('time', 10)\n",
    "        ds.createDimension('lat', 30)\n",
    "        ds.createDimension('lon', 40)\n",
    "        \n",
    "        var = ds.createVariable(ds_info['var'], 'f4', ('time', 'lat', 'lon'))\n",
    "        var[:] = np.random.randn(10, 30, 40) + ds_info['value']\n",
    "    \n",
    "    print(f\"  âœ“ {ds_info['name']}\")\n",
    "\n",
    "# Create README\n",
    "with open(test_dir / \"README.md\", 'w') as f:\n",
    "    f.write(\"# Test Dataset Collection\\n\\nOcean and atmospheric data for 2023.\")\n",
    "\n",
    "print(f\"\\nâœ“ Created {len(datasets)} files in {test_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Create Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created archive: research_data_2023.zip\n",
      "  Size: 200.7 KB\n",
      "\n",
      "Archive contents:\n",
      "  Total files: 6\n",
      "  Data files: 5\n",
      "  Documentation: 1\n",
      "\n",
      "Data files in archive:\n",
      "    - data/ocean_temp_feb_2023.nc\n",
      "    - data/wind_speed_feb_2023.nc\n",
      "    - data/ocean_temp_jan_2023.nc\n",
      "    - data/wind_speed_jan_2023.nc\n",
      "    - data/ocean_temp_mar_2023.nc\n"
     ]
    }
   ],
   "source": [
    "# Create a .zip archive\n",
    "archive_path = Path(\"research_data_2023.zip\")\n",
    "\n",
    "with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    # Add all .nc files\n",
    "    for nc_file in test_dir.glob(\"*.nc\"):\n",
    "        zf.write(nc_file, f\"data/{nc_file.name}\")\n",
    "    \n",
    "    # Add README\n",
    "    readme = test_dir / \"README.md\"\n",
    "    if readme.exists():\n",
    "        zf.write(readme, \"README.md\")\n",
    "\n",
    "print(f\"âœ“ Created archive: {archive_path}\")\n",
    "print(f\"  Size: {archive_path.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Inspect archive structure\n",
    "handler = ArchiveHandler()\n",
    "structure = handler.get_archive_structure(archive_path)\n",
    "\n",
    "print(f\"\\nArchive contents:\")\n",
    "print(f\"  Total files: {len(structure['files'])}\")\n",
    "print(f\"  Data files: {len(structure['data_files'])}\")\n",
    "print(f\"  Documentation: {len(structure['companion_files'])}\")\n",
    "print(f\"\\nData files in archive:\")\n",
    "for df in structure['data_files']:\n",
    "    print(f\"    - {df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Batch Validate Before Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating files...\n",
      "\n",
      "Validation Results:\n",
      "============================================================\n",
      "Total files: 5\n",
      "âœ“ Valid: 5\n",
      "âœ— Invalid: 0\n",
      "\n",
      "âœ“ All files passed validation!\n"
     ]
    }
   ],
   "source": [
    "# Always validate before indexing\n",
    "validator = FileValidator()\n",
    "\n",
    "print(\"Validating files...\")\n",
    "results = validator.validate_directory(test_dir)\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total files: {results['total_files']}\")\n",
    "print(f\"âœ“ Valid: {len(results['valid'])}\")\n",
    "print(f\"âœ— Invalid: {len(results['invalid'])}\")\n",
    "\n",
    "if results['invalid']:\n",
    "    print(\"\\nInvalid files:\")\n",
    "    for inv in results['invalid']:\n",
    "        print(f\"  - {Path(inv['filepath']).name}\")\n",
    "        print(f\"    Issues: {inv['issues']}\")\n",
    "else:\n",
    "    print(\"\\nâœ“ All files passed validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Batch Index Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing search engine...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Model loaded. Embedding dimension: 384\n",
      "Loaded embedding cache: 10 entries\n",
      "Creating new index...\n",
      "Initialized FAISS index (dim=384)\n",
      "\n",
      "Indexing directory: batch_test_data\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing files: 100%|â–ˆ| 5/5 [00:00<00:00, 42."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing Results:\n",
      "============================================================\n",
      "âœ“ Successfully indexed: 5\n",
      "âœ— Errors: 0\n",
      "â±ï¸  Time: 0.15 seconds\n",
      "ðŸ“Š Speed: 34.4 files/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize search engine\n",
    "print(\"Initializing search engine...\")\n",
    "engine = FAIRSearchEngine(load_existing=False)\n",
    "\n",
    "# Batch index the directory\n",
    "print(f\"\\nIndexing directory: {test_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "result = engine.index_directory(\n",
    "    test_dir,\n",
    "    validate=True,\n",
    "    include_companions=True,\n",
    "    extract_archives=False,  # Just files, not archives yet\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(\"\\nIndexing Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âœ“ Successfully indexed: {result['indexed']}\")\n",
    "print(f\"âœ— Errors: {result['errors']}\")\n",
    "print(f\"â±ï¸  Time: {elapsed:.2f} seconds\")\n",
    "print(f\"ðŸ“Š Speed: {result['indexed'] / elapsed:.1f} files/second\")\n",
    "\n",
    "if result['errors'] > 0:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in result['details']['errors'][:5]:\n",
    "        print(f\"  - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Index Archive (Auto-Extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing archive: research_data_2023.zip\n",
      "============================================================\n",
      "\n",
      "Archive Indexing Results:\n",
      "============================================================\n",
      "Archives processed: 1\n",
      "Files indexed: 5\n",
      "Errors: 0\n",
      "\n",
      "Indexed files from archive:\n",
      "  - ocean_temp_feb_2023.nc\n",
      "    From: research_data_2023.zip\n",
      "    Path: data/wind_speed_jan_2023.nc\n",
      "  - ocean_temp_jan_2023.nc\n",
      "    From: research_data_2023.zip\n",
      "    Path: data/wind_speed_jan_2023.nc\n",
      "  - ocean_temp_mar_2023.nc\n",
      "    From: research_data_2023.zip\n",
      "    Path: data/wind_speed_jan_2023.nc\n",
      "  - wind_speed_feb_2023.nc\n",
      "    From: research_data_2023.zip\n",
      "    Path: data/wind_speed_jan_2023.nc\n",
      "  - wind_speed_jan_2023.nc\n",
      "    From: research_data_2023.zip\n",
      "    Path: data/wind_speed_jan_2023.nc\n",
      "\n",
      "âœ“ Added 5 files from archive to index\n"
     ]
    }
   ],
   "source": [
    "# Index the archive - it will auto-extract\n",
    "print(f\"Indexing archive: {archive_path}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create archive-aware indexer\n",
    "indexer = ArchiveAwareIndexer(\n",
    "    metadata_extractor=MetadataExtractor(),\n",
    "    companion_finder=CompanionDocFinder()\n",
    ")\n",
    "\n",
    "# Index the archive\n",
    "archive_results = indexer.index_path(archive_path, extract_archives=True)\n",
    "\n",
    "print(\"\\nArchive Indexing Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Archives processed: {len(archive_results['archives_processed'])}\")\n",
    "print(f\"Files indexed: {len(archive_results['indexed_files'])}\")\n",
    "print(f\"Errors: {len(archive_results['errors'])}\")\n",
    "\n",
    "print(\"\\nIndexed files from archive:\")\n",
    "for meta in archive_results['indexed_files'][:5]:  # Show first 5\n",
    "    filename = Path(meta['filepath']).name\n",
    "    print(f\"  - {filename}\")\n",
    "    if 'archive_context' in meta:\n",
    "        ctx = meta['archive_context']\n",
    "        print(f\"    From: {ctx['from_archive']}\")\n",
    "        print(f\"    Path: {ctx['relative_path']}\")\n",
    "\n",
    "# Add to main index\n",
    "for meta in archive_results['indexed_files']:\n",
    "    searchable_text = engine.metadata_extractor.create_searchable_text(meta)\n",
    "    embedding = engine.embedding_generator.encode_single(searchable_text)\n",
    "    engine.vector_index.add(embedding.reshape(1, -1), [meta])\n",
    "\n",
    "print(f\"\\nâœ“ Added {len(archive_results['indexed_files'])} files from archive to index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Search Across All Indexed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: 'ocean temperature'\n",
      "============================================================\n",
      "\n",
      "1. ocean_temp_feb_2023.nc\n",
      "   Score: 0.548\n",
      "   Title: Test temperature data\n",
      "   ðŸ“¦ From: research_data_2023.zip\n",
      "\n",
      "2. ocean_temp_jan_2023.nc\n",
      "   Score: 0.529\n",
      "   Title: Test temperature data\n",
      "   ðŸ“¦ From: research_data_2023.zip\n",
      "\n",
      "3. ocean_temp_mar_2023.nc\n",
      "   Score: 0.524\n",
      "   Title: Test temperature data\n",
      "   ðŸ“¦ From: research_data_2023.zip\n",
      "\n",
      "============================================================\n",
      "Query: 'wind data'\n",
      "============================================================\n",
      "\n",
      "1. wind_speed_feb_2023.nc\n",
      "   Score: 0.581\n",
      "   Title: Test wind data\n",
      "   ðŸ“¦ From: research_data_2023.zip\n",
      "\n",
      "2. wind_speed_jan_2023.nc\n",
      "   Score: 0.558\n",
      "   Title: Test wind data\n",
      "   ðŸ“¦ From: research_data_2023.zip\n",
      "\n",
      "3. wind_speed_feb_2023.nc\n",
      "   Score: 0.540\n",
      "   Title: Test wind data\n",
      "\n",
      "============================================================\n",
      "Query: 'january 2023'\n",
      "============================================================\n",
      "\n",
      "1. ocean_temp_feb_2023.nc\n",
      "   Score: 0.285\n",
      "   Title: Test temperature data\n",
      "\n",
      "2. ocean_temp_jan_2023.nc\n",
      "   Score: 0.272\n",
      "   Title: Test temperature data\n",
      "\n",
      "3. ocean_temp_feb_2023.nc\n",
      "   Score: 0.240\n",
      "   Title: Test temperature data\n",
      "   ðŸ“¦ From: research_data_2023.zip\n"
     ]
    }
   ],
   "source": [
    "# Test searches\n",
    "queries = [\n",
    "    \"ocean temperature\",\n",
    "    \"wind data\",\n",
    "    \"january 2023\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print('='*60)\n",
    "    \n",
    "    results = engine.search(query, top_k=3)\n",
    "    \n",
    "    if results:\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {Path(result['filepath']).name}\")\n",
    "            print(f\"   Score: {result['similarity_score']:.3f}\")\n",
    "            print(f\"   Title: {result.get('title', 'N/A')}\")\n",
    "            \n",
    "            # Show if from archive\n",
    "            if 'archive_context' in result:\n",
    "                ctx = result['archive_context']\n",
    "                print(f\"   ðŸ“¦ From: {ctx['from_archive']}\")\n",
    "    else:\n",
    "        print(\"No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Incremental Updates\n",
    "\n",
    "Add new files to existing index without rebuilding everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving index...\n",
      "Index saved to /home/vastdata/vast-fair-stack/lib/indexes/faiss_index.bin\n",
      "Index contains 10 datasets\n",
      "\n",
      "âœ“ Created new file: ocean_temp_apr_2023.nc\n",
      "\n",
      "Loading existing index...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Model loaded. Embedding dimension: 384\n",
      "Loaded embedding cache: 22 entries\n",
      "Loading existing index...\n",
      "Initialized FAISS index (dim=384)\n",
      "Index loaded: {'total_vectors': 10, 'total_metadata': 10, 'unique_files': 10, 'embedding_dim': 384, 'index_type': 'IndexFlatIP'}\n",
      "Adding new file to index...\n",
      "âœ“ Added: ocean_temp_apr_2023.nc\n",
      "Index saved to /home/vastdata/vast-fair-stack/lib/indexes/faiss_index.bin\n",
      "\n",
      "Index now contains 11 datasets\n",
      "Added: 1 new file(s)\n"
     ]
    }
   ],
   "source": [
    "# Save current index\n",
    "print(\"Saving index...\")\n",
    "engine.save()\n",
    "\n",
    "stats_before = engine.get_stats()\n",
    "print(f\"Index contains {stats_before['total_vectors']} datasets\")\n",
    "\n",
    "# Create a new file\n",
    "new_file = test_dir / \"ocean_temp_apr_2023.nc\"\n",
    "with netCDF4.Dataset(new_file, 'w') as ds:\n",
    "    ds.title = \"April 2023 Temperature\"\n",
    "    ds.institution = \"Demo Lab\"\n",
    "    \n",
    "    ds.createDimension('time', 10)\n",
    "    ds.createDimension('lat', 30)\n",
    "    ds.createDimension('lon', 40)\n",
    "    \n",
    "    var = ds.createVariable('temperature', 'f4', ('time', 'lat', 'lon'))\n",
    "    var[:] = np.random.randn(10, 30, 40) + 18\n",
    "\n",
    "print(f\"\\nâœ“ Created new file: {new_file.name}\")\n",
    "\n",
    "# Load existing index and add new file\n",
    "print(\"\\nLoading existing index...\")\n",
    "engine2 = FAIRSearchEngine(load_existing=True)\n",
    "\n",
    "print(\"Adding new file to index...\")\n",
    "result = engine2.index_file(new_file)\n",
    "\n",
    "if result.get('success'):\n",
    "    print(f\"âœ“ Added: {new_file.name}\")\n",
    "\n",
    "# Save updated index\n",
    "engine2.save()\n",
    "\n",
    "stats_after = engine2.get_stats()\n",
    "print(f\"\\nIndex now contains {stats_after['total_vectors']} datasets\")\n",
    "print(f\"Added: {stats_after['total_vectors'] - stats_before['total_vectors']} new file(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking search performance...\n",
      "============================================================\n",
      "Ran 100 searches in 0.02 seconds\n",
      "\n",
      "Performance:\n",
      "  Average search time: 0.16 ms\n",
      "  Searches per second: 6130\n",
      "  Target: <200ms per search - âœ“ PASS\n",
      "\n",
      "Index Statistics:\n",
      "  Total datasets: 11\n",
      "  Unique files: 11\n",
      "  Cache size: 24 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Benchmark search speed\n",
    "print(\"Benchmarking search performance...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"ocean temperature data\"\n",
    "num_searches = 100\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(num_searches):\n",
    "    engine2.search(query, top_k=10)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "avg_time_ms = (elapsed / num_searches) * 1000\n",
    "searches_per_sec = num_searches / elapsed\n",
    "\n",
    "print(f\"Ran {num_searches} searches in {elapsed:.2f} seconds\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Average search time: {avg_time_ms:.2f} ms\")\n",
    "print(f\"  Searches per second: {searches_per_sec:.0f}\")\n",
    "print(f\"  Target: <200ms per search - {'âœ“ PASS' if avg_time_ms < 200 else 'âœ— FAIL'}\")\n",
    "\n",
    "# Index size\n",
    "stats = engine2.get_stats()\n",
    "print(f\"\\nIndex Statistics:\")\n",
    "print(f\"  Total datasets: {stats['total_vectors']}\")\n",
    "print(f\"  Unique files: {stats['unique_files']}\")\n",
    "print(f\"  Cache size: {stats['cache_size']} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 9: Error Handling and Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing error handling with corrupt file...\n",
      "============================================================\n",
      "âœ“ Error detected correctly: Invalid: File too small (31 bytes)\n",
      "âœ“ System continues running (no crash)\n",
      "\n",
      "âœ“ Error handling test complete\n"
     ]
    }
   ],
   "source": [
    "# Create a corrupt file to test error handling\n",
    "corrupt_file = test_dir / \"corrupt_data.nc\"\n",
    "with open(corrupt_file, 'w') as f:\n",
    "    f.write(\"This is not a valid NetCDF file\")\n",
    "\n",
    "print(\"Testing error handling with corrupt file...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Try to index - should handle gracefully\n",
    "result = engine2.index_file(corrupt_file, validate=True)\n",
    "\n",
    "if result.get('error'):\n",
    "    print(f\"âœ“ Error detected correctly: {result['error']}\")\n",
    "    print(\"âœ“ System continues running (no crash)\")\n",
    "else:\n",
    "    print(\"âš ï¸  File was indexed (should have been rejected)\")\n",
    "\n",
    "# Clean up\n",
    "corrupt_file.unlink()\n",
    "print(\"\\nâœ“ Error handling test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 10: Production Workflow Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production Workflow Summary\n",
      "============================================================\n",
      "\n",
      "1. VALIDATE\n",
      "   â””â”€ Check file signatures before processing\n",
      "   â””â”€ Identify corrupt/invalid files early\n",
      "\n",
      "2. EXTRACT METADATA\n",
      "   â””â”€ Handle minimal metadata gracefully\n",
      "   â””â”€ Find companion documentation\n",
      "\n",
      "3. PROCESS ARCHIVES\n",
      "   â””â”€ Auto-extract .zip, .tar.gz files\n",
      "   â””â”€ Preserve archive context\n",
      "\n",
      "4. BATCH INDEX\n",
      "   â””â”€ Process directories efficiently\n",
      "   â””â”€ Show progress for long operations\n",
      "   â””â”€ Handle errors without crashing\n",
      "\n",
      "5. INCREMENTAL UPDATES\n",
      "   â””â”€ Add new files without rebuilding\n",
      "   â””â”€ Fast updates for ongoing projects\n",
      "\n",
      "6. VERIFY\n",
      "   â””â”€ Test search functionality\n",
      "   â””â”€ Benchmark performance\n",
      "   â””â”€ Save index for reuse\n",
      "\n",
      "Final Index State:\n",
      "  ðŸ“Š Total datasets: 11\n",
      "  ðŸ“ Unique files: 11\n",
      "  ðŸ¤– Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  ðŸ’¾ Cache: 24 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Summary of production workflow\n",
    "print(\"Production Workflow Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "1. VALIDATE\n",
    "   â””â”€ Check file signatures before processing\n",
    "   â””â”€ Identify corrupt/invalid files early\n",
    "   \n",
    "2. EXTRACT METADATA\n",
    "   â””â”€ Handle minimal metadata gracefully\n",
    "   â””â”€ Find companion documentation\n",
    "   \n",
    "3. PROCESS ARCHIVES\n",
    "   â””â”€ Auto-extract .zip, .tar.gz files\n",
    "   â””â”€ Preserve archive context\n",
    "   \n",
    "4. BATCH INDEX\n",
    "   â””â”€ Process directories efficiently\n",
    "   â””â”€ Show progress for long operations\n",
    "   â””â”€ Handle errors without crashing\n",
    "   \n",
    "5. INCREMENTAL UPDATES\n",
    "   â””â”€ Add new files without rebuilding\n",
    "   â””â”€ Fast updates for ongoing projects\n",
    "   \n",
    "6. VERIFY\n",
    "   â””â”€ Test search functionality\n",
    "   â””â”€ Benchmark performance\n",
    "   â””â”€ Save index for reuse\n",
    "\"\"\")\n",
    "\n",
    "final_stats = engine2.get_stats()\n",
    "print(\"Final Index State:\")\n",
    "print(f\"  ðŸ“Š Total datasets: {final_stats['total_vectors']}\")\n",
    "print(f\"  ðŸ“ Unique files: {final_stats['unique_files']}\")\n",
    "print(f\"  ðŸ¤– Model: {final_stats['model']}\")\n",
    "print(f\"  ðŸ’¾ Cache: {final_stats['cache_size']} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Production\n",
    "\n",
    "### âœ… Do This\n",
    "1. **Always validate first** - Catch bad files early\n",
    "2. **Use progress bars** - Monitor long operations\n",
    "3. **Handle errors gracefully** - Log but don't crash\n",
    "4. **Incremental updates** - Don't rebuild everything\n",
    "5. **Test search** - Verify quality after indexing\n",
    "\n",
    "### âš ï¸ Avoid This\n",
    "1. Indexing without validation\n",
    "2. Processing archives without checking contents\n",
    "3. Ignoring error messages\n",
    "4. Rebuilding index for small updates\n",
    "5. Not testing search quality\n",
    "\n",
    "## Performance Expectations\n",
    "\n",
    "| Operation | Speed | Notes |\n",
    "|-----------|-------|-------|\n",
    "| Validation | 1000+ files/sec | Very fast |\n",
    "| Indexing | 5-15 files/min | Depends on file size |\n",
    "| Archive extraction | Varies | Depends on size |\n",
    "| Search | <200ms | Fast even with 10k files |\n",
    "\n",
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up test files\n",
    "# import shutil\n",
    "# shutil.rmtree(test_dir)\n",
    "# archive_path.unlink()\n",
    "# print(\"âœ“ Test files cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Notebook 06**: Optional LLM enrichment for enhanced metadata\n",
    "- **Notebook 99**: Complete end-to-end workflows\n",
    "\n",
    "## Command-Line Usage\n",
    "\n",
    "For production, use the CLI tools:\n",
    "\n",
    "```bash\n",
    "# Batch index\n",
    "python fair_index.py index /path/to/data --extract-archives\n",
    "\n",
    "# With validation\n",
    "python fair_index.py validate /path/to/data\n",
    "python fair_index.py index /path/to/data\n",
    "\n",
    "# Check progress\n",
    "python fair_index.py stats\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
