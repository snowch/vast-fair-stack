{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional LLM Enrichment for Enhanced Metadata\n",
    "\n",
    "This notebook demonstrates how to use a local LLM (Ollama) to enhance metadata extraction.\n",
    "\n",
    "## Why LLM Enrichment?\n",
    "\n",
    "LLMs can:\n",
    "- **Decode abbreviations**: sst ‚Üí sea surface temperature\n",
    "- **Infer context**: Guess institution from project names\n",
    "- **Suggest use cases**: Identify potential applications\n",
    "- **Add domain knowledge**: Recognize scientific domain\n",
    "- **Generate summaries**: Create human-readable descriptions\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- ‚úÖ **Completely Optional** - System works fine without it\n",
    "- ‚úÖ **Privacy Preserving** - Uses local Ollama, no data sent externally\n",
    "- ‚ö†Ô∏è **Slower** - Adds ~2-5 seconds per file\n",
    "- ‚ö†Ô∏è **Requires Ollama** - Install separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Ollama\n",
    "\n",
    "First, install Ollama if you haven't already:\n",
    "\n",
    "```bash\n",
    "# Install Ollama\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Pull a model (choose one)\n",
    "ollama pull llama3.2:3b      # Fast, 2GB, good quality\n",
    "ollama pull llama3.2:8b      # Slower, 4GB, better quality\n",
    "ollama pull mistral:7b       # Alternative\n",
    "\n",
    "# Test it\n",
    "ollama run llama3.2:3b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from llm_enricher import LLMEnricher, DataInspector\n",
    "from metadata_extractors import MetadataExtractor\n",
    "from search_engine import FAIRSearchEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Test Ollama Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection to Ollama\n",
    "try:\n",
    "    enricher = LLMEnricher(model=\"llama3.2:3b\")\n",
    "    print(\"‚úì Connected to Ollama\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Could not connect to Ollama: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running:\")\n",
    "    print(\"  ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Enrich Basic Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract basic metadata first\n",
    "sample_file = Path(\"sample_data/ocean_temperature.nc\")\n",
    "\n",
    "if sample_file.exists():\n",
    "    extractor = MetadataExtractor()\n",
    "    metadata = extractor.extract(sample_file)\n",
    "    \n",
    "    print(\"Original Metadata:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Filename: {metadata.get('filename')}\")\n",
    "    print(f\"Title: {metadata.get('title')}\")\n",
    "    print(f\"Institution: {metadata.get('institution')}\")\n",
    "    print(f\"Variables: {list(metadata.get('variables', {}).keys())}\")\n",
    "else:\n",
    "    print(\"Sample file not found. Run notebook 00 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich with LLM\n",
    "if sample_file.exists():\n",
    "    print(\"\\nEnriching metadata with LLM...\")\n",
    "    print(\"(This may take a few seconds)\\n\")\n",
    "    \n",
    "    enriched = enricher.enrich_metadata(metadata)\n",
    "    \n",
    "    if 'llm_enrichment' in enriched:\n",
    "        llm_data = enriched['llm_enrichment']\n",
    "        \n",
    "        print(\"LLM-Enriched Metadata:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if 'domain' in llm_data:\n",
    "            print(f\"Domain: {llm_data['domain']}\")\n",
    "        \n",
    "        if 'source_institution' in llm_data:\n",
    "            print(f\"Inferred Institution: {llm_data['source_institution']}\")\n",
    "        \n",
    "        if 'variable_descriptions' in llm_data:\n",
    "            print(f\"\\nVariable Descriptions:\")\n",
    "            for var, desc in llm_data['variable_descriptions'].items():\n",
    "                print(f\"  {var}: {desc}\")\n",
    "        \n",
    "        if 'use_cases' in llm_data:\n",
    "            print(f\"\\nPotential Use Cases:\")\n",
    "            for use_case in llm_data['use_cases']:\n",
    "                print(f\"  - {use_case}\")\n",
    "        \n",
    "        if 'quality_notes' in llm_data:\n",
    "            print(f\"\\nQuality Notes: {llm_data['quality_notes']}\")\n",
    "    else:\n",
    "        print(f\"Enrichment error: {enriched.get('llm_error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Data Inspection Tools\n",
    "\n",
    "Inspect actual data values to validate metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect variable statistics\n",
    "if sample_file.exists():\n",
    "    inspector = DataInspector()\n",
    "    \n",
    "    print(\"Variable Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    stats = inspector.get_variable_statistics(\n",
    "        str(sample_file), \n",
    "        'sea_surface_temperature'\n",
    "    )\n",
    "    \n",
    "    if 'error' not in stats:\n",
    "        print(f\"Variable: sea_surface_temperature\")\n",
    "        print(f\"  Min: {stats['min']:.2f}\")\n",
    "        print(f\"  Max: {stats['max']:.2f}\")\n",
    "        print(f\"  Mean: {stats['mean']:.2f}\")\n",
    "        print(f\"  Std Dev: {stats['std']:.2f}\")\n",
    "        print(f\"  Shape: {stats['shape']}\")\n",
    "        print(f\"  Data Type: {stats['dtype']}\")\n",
    "    else:\n",
    "        print(f\"Error: {stats['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check temporal coverage\n",
    "if sample_file.exists():\n",
    "    print(\"\\nTemporal Coverage:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    temporal = inspector.check_temporal_coverage(str(sample_file))\n",
    "    \n",
    "    if 'error' not in temporal:\n",
    "        print(f\"Number of timesteps: {temporal['num_timesteps']}\")\n",
    "        print(f\"Time units: {temporal['units']}\")\n",
    "        \n",
    "        if 'start_date' in temporal:\n",
    "            print(f\"Start: {temporal['start_date']}\")\n",
    "            print(f\"End: {temporal['end_date']}\")\n",
    "    else:\n",
    "        print(f\"Error: {temporal['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check spatial coverage\n",
    "if sample_file.exists():\n",
    "    print(\"\\nSpatial Coverage:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    spatial = inspector.check_spatial_coverage(str(sample_file))\n",
    "    \n",
    "    if 'error' not in spatial:\n",
    "        print(f\"Latitude: {spatial['lat_min']:.2f}¬∞ to {spatial['lat_max']:.2f}¬∞\")\n",
    "        print(f\"Longitude: {spatial['lon_min']:.2f}¬∞ to {spatial['lon_max']:.2f}¬∞\")\n",
    "        \n",
    "        if spatial['lat_resolution']:\n",
    "            print(f\"Lat resolution: {spatial['lat_resolution']:.2f}¬∞\")\n",
    "        if spatial['lon_resolution']:\n",
    "            print(f\"Lon resolution: {spatial['lon_resolution']:.2f}¬∞\")\n",
    "    else:\n",
    "        print(f\"Error: {spatial['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Enrich Search Results\n",
    "\n",
    "Add LLM-generated summaries to search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search first\n",
    "try:\n",
    "    engine = FAIRSearchEngine(load_existing=True)\n",
    "    results = engine.search(\"ocean temperature\", top_k=3)\n",
    "    \n",
    "    if results:\n",
    "        print(\"Original Search Results:\")\n",
    "        print(\"=\" * 60)\n",
    "        for i, r in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {Path(r['filepath']).name}\")\n",
    "            print(f\"   Score: {r['similarity_score']:.3f}\")\n",
    "            print(f\"   Title: {r.get('title', 'N/A')}\")\n",
    "        \n",
    "        # Enrich with LLM summaries\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Generating LLM Summaries...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        enriched_results = enricher.enrich_search_results(results)\n",
    "        \n",
    "        print(\"\\nEnriched Results:\")\n",
    "        print(\"=\" * 60)\n",
    "        for i, r in enumerate(enriched_results, 1):\n",
    "            print(f\"\\n{i}. {Path(r['filepath']).name}\")\n",
    "            print(f\"   Score: {r['similarity_score']:.3f}\")\n",
    "            \n",
    "            if 'llm_summary' in r:\n",
    "                print(f\"   Summary: {r['llm_summary']}\")\n",
    "    else:\n",
    "        print(\"No results found. Index some data first.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"No index found. Run indexing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Compare With/Without Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimal metadata file\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "\n",
    "minimal_file = Path(\"sample_data/test_minimal.nc\")\n",
    "\n",
    "with netCDF4.Dataset(minimal_file, 'w') as ds:\n",
    "    # NO global attributes, only variable name\n",
    "    ds.createDimension('x', 100)\n",
    "    ds.createDimension('y', 100)\n",
    "    ds.createDimension('t', 10)\n",
    "    \n",
    "    var = ds.createVariable('wspd', 'f4', ('t', 'y', 'x'))\n",
    "    var[:] = np.random.randn(10, 100, 100) * 5 + 10\n",
    "\n",
    "print(f\"Created minimal file: {minimal_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract without enrichment\n",
    "extractor = MetadataExtractor()\n",
    "basic_metadata = extractor.extract(minimal_file)\n",
    "\n",
    "print(\"Without LLM Enrichment:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Title: {basic_metadata.get('title') or '(none)'}\")\n",
    "print(f\"Institution: {basic_metadata.get('institution') or '(none)'}\")\n",
    "print(f\"Variables: {list(basic_metadata.get('variables', {}).keys())}\")\n",
    "print(f\"\\nSearchable text length: {len(extractor.create_searchable_text(basic_metadata))} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich with LLM\n",
    "enriched_metadata = enricher.enrich_metadata(basic_metadata)\n",
    "\n",
    "print(\"\\nWith LLM Enrichment:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'llm_enrichment' in enriched_metadata:\n",
    "    llm_data = enriched_metadata['llm_enrichment']\n",
    "    \n",
    "    print(f\"Inferred domain: {llm_data.get('domain', 'N/A')}\")\n",
    "    print(f\"\\nVariable interpretation:\")\n",
    "    for var, desc in llm_data.get('variable_descriptions', {}).items():\n",
    "        print(f\"  {var} ‚Üí {desc}\")\n",
    "    \n",
    "    print(f\"\\nPotential uses:\")\n",
    "    for use in llm_data.get('use_cases', []):\n",
    "        print(f\"  - {use}\")\n",
    "    \n",
    "    # Create enriched searchable text\n",
    "    enriched_text = extractor.create_searchable_text(basic_metadata)\n",
    "    enriched_text += \" \" + str(llm_data)\n",
    "    print(f\"\\nSearchable text length: {len(enriched_text)} chars (increased by {len(enriched_text) - len(extractor.create_searchable_text(basic_metadata))}\")\n",
    "else:\n",
    "    print(f\"Error: {enriched_metadata.get('llm_error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Time basic extraction\n",
    "start = time.time()\n",
    "basic = extractor.extract(sample_file)\n",
    "basic_time = time.time() - start\n",
    "\n",
    "# Time with LLM enrichment\n",
    "start = time.time()\n",
    "enriched = enricher.enrich_metadata(basic)\n",
    "llm_time = time.time() - start\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Basic extraction: {basic_time:.3f} seconds\")\n",
    "print(f\"LLM enrichment: {llm_time:.3f} seconds\")\n",
    "print(f\"Total with LLM: {basic_time + llm_time:.3f} seconds\")\n",
    "print(f\"\\nSpeed difference: {((basic_time + llm_time) / basic_time):.1f}x slower with LLM\")\n",
    "\n",
    "print(\"\\nüí° Recommendation:\")\n",
    "if llm_time < 5:\n",
    "    print(\"  LLM enrichment is reasonably fast for your use case\")\n",
    "else:\n",
    "    print(\"  Consider using LLM enrichment selectively for important datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use LLM Enrichment\n",
    "\n",
    "### ‚úÖ Good Use Cases\n",
    "- Files with minimal or cryptic metadata\n",
    "- Variable names that are abbreviations\n",
    "- Data from unfamiliar sources\n",
    "- High-value datasets worth extra processing\n",
    "- Interactive exploration (real-time is fine)\n",
    "\n",
    "### ‚ö†Ô∏è Consider Skipping\n",
    "- Large batch processing (1000s of files)\n",
    "- Well-documented CF-compliant files\n",
    "- Time-sensitive indexing workflows\n",
    "- When basic search already works well\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Index without LLM first** - Get basic search working quickly\n",
    "2. **Enrich selectively** - Use LLM for important datasets\n",
    "3. **Cache results** - Store enriched metadata\n",
    "4. **Use local models** - Keep data private\n",
    "5. **Monitor quality** - Verify LLM outputs make sense\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 99**: See complete workflows with optional enrichment\n",
    "- Try different Ollama models for quality/speed tradeoffs\n",
    "- Integrate enrichment into your indexing pipeline selectively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
