{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional LLM Enrichment for Enhanced Metadata\n",
    "\n",
    "This notebook demonstrates how to use a local LLM (Ollama) to enhance metadata extraction.\n",
    "\n",
    "## Why LLM Enrichment?\n",
    "\n",
    "LLMs can:\n",
    "- **Decode abbreviations**: sst → sea surface temperature\n",
    "- **Infer context**: Guess institution from project names\n",
    "- **Suggest use cases**: Identify potential applications\n",
    "- **Add domain knowledge**: Recognize scientific domain\n",
    "- **Generate summaries**: Create human-readable descriptions\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- ✅ **Completely Optional** - System works fine without it\n",
    "- ✅ **Privacy Preserving** - Uses local Ollama, no data sent externally\n",
    "- ⚠️ **Slower** - Adds ~2-5 seconds per file\n",
    "- ⚠️ **Requires Ollama** - Install separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Ollama\n",
    "\n",
    "First, install Ollama if you haven't already:\n",
    "\n",
    "```bash\n",
    "# Install Ollama\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Pull a model (choose one)\n",
    "ollama pull llama3.2:3b      # Fast, 2GB, good quality\n",
    "# ollama pull llama3.2:8b      # Slower, 4GB, better quality\n",
    "# ollama pull mistral:7b       # Alternative\n",
    "\n",
    "# Test it\n",
    "ollama run llama3.2:3b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from llm_enricher import LLMEnricher, DataInspector\n",
    "from metadata_extractors import MetadataExtractor\n",
    "from search_engine import FAIRSearchEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Test Ollama Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Ollama at http://localhost:11434\n",
      "✓ Connected to Ollama\n"
     ]
    }
   ],
   "source": [
    "# Test connection to Ollama\n",
    "try:\n",
    "    enricher = LLMEnricher(model=\"llama3.2:3b\")\n",
    "    print(\"✓ Connected to Ollama\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Could not connect to Ollama: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running:\")\n",
    "    print(\"  ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Enrich Basic Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Metadata:\n",
      "============================================================\n",
      "Filename: ocean_temperature.nc\n",
      "Title: Sample Ocean Temperature Data\n",
      "Institution: Demo University\n",
      "Variables: ['time', 'lat', 'lon', 'sea_surface_temperature']\n"
     ]
    }
   ],
   "source": [
    "# Extract basic metadata first\n",
    "sample_file = Path(\"sample_data/ocean_temperature.nc\")\n",
    "\n",
    "if sample_file.exists():\n",
    "    extractor = MetadataExtractor()\n",
    "    metadata = extractor.extract(sample_file)\n",
    "    \n",
    "    print(\"Original Metadata:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Filename: {metadata.get('filename')}\")\n",
    "    print(f\"Title: {metadata.get('title')}\")\n",
    "    print(f\"Institution: {metadata.get('institution')}\")\n",
    "    print(f\"Variables: {list(metadata.get('variables', {}).keys())}\")\n",
    "else:\n",
    "    print(\"Sample file not found. Run notebook 00 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enriching metadata with LLM...\n",
      "(This may take a few seconds)\n",
      "\n",
      "LLM-Enriched Metadata:\n",
      "============================================================\n",
      "Domain: Oceanography\n",
      "Inferred Institution: Demo University\n",
      "\n",
      "Variable Descriptions:\n",
      "  time: Date and time of observation (e.g., year, month, day, hour)\n",
      "  lat: Latitude coordinate (-90° to +90° N/S)\n",
      "  lon: Longitude coordinate (-180° to +180° E/W)\n",
      "  sea_surface_temperature: Temperature at the sea surface in degrees Celsius\n",
      "\n",
      "Potential Use Cases:\n",
      "  - Climate modeling and analysis\n",
      "  - Ocean current research and tracking\n",
      "  - Sea ice formation and melting studies\n",
      "\n",
      "Quality Notes: \n"
     ]
    }
   ],
   "source": [
    "# Enrich with LLM\n",
    "if sample_file.exists():\n",
    "    print(\"\\nEnriching metadata with LLM...\")\n",
    "    print(\"(This may take a few seconds)\\n\")\n",
    "    \n",
    "    enriched = enricher.enrich_metadata(metadata)\n",
    "    \n",
    "    if 'llm_enrichment' in enriched:\n",
    "        llm_data = enriched['llm_enrichment']\n",
    "        \n",
    "        print(\"LLM-Enriched Metadata:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if 'domain' in llm_data:\n",
    "            print(f\"Domain: {llm_data['domain']}\")\n",
    "        \n",
    "        if 'source_institution' in llm_data:\n",
    "            print(f\"Inferred Institution: {llm_data['source_institution']}\")\n",
    "        \n",
    "        if 'variable_descriptions' in llm_data:\n",
    "            print(f\"\\nVariable Descriptions:\")\n",
    "            for var, desc in llm_data['variable_descriptions'].items():\n",
    "                print(f\"  {var}: {desc}\")\n",
    "        \n",
    "        if 'use_cases' in llm_data:\n",
    "            print(f\"\\nPotential Use Cases:\")\n",
    "            for use_case in llm_data['use_cases']:\n",
    "                print(f\"  - {use_case}\")\n",
    "        \n",
    "        if 'quality_notes' in llm_data:\n",
    "            print(f\"\\nQuality Notes: {llm_data['quality_notes']}\")\n",
    "    else:\n",
    "        print(f\"Enrichment error: {enriched.get('llm_error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Data Inspection Tools\n",
    "\n",
    "Inspect actual data values to validate metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Statistics:\n",
      "============================================================\n",
      "Variable: sea_surface_temperature\n",
      "  Min: -2.41\n",
      "  Max: 35.17\n",
      "  Mean: 14.91\n",
      "  Std Dev: 4.97\n",
      "  Shape: (10, 20, 30)\n",
      "  Data Type: float32\n"
     ]
    }
   ],
   "source": [
    "# Inspect variable statistics\n",
    "if sample_file.exists():\n",
    "    inspector = DataInspector()\n",
    "    \n",
    "    print(\"Variable Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    stats = inspector.get_variable_statistics(\n",
    "        str(sample_file), \n",
    "        'sea_surface_temperature'\n",
    "    )\n",
    "    \n",
    "    if 'error' not in stats:\n",
    "        print(f\"Variable: sea_surface_temperature\")\n",
    "        print(f\"  Min: {stats['min']:.2f}\")\n",
    "        print(f\"  Max: {stats['max']:.2f}\")\n",
    "        print(f\"  Mean: {stats['mean']:.2f}\")\n",
    "        print(f\"  Std Dev: {stats['std']:.2f}\")\n",
    "        print(f\"  Shape: {stats['shape']}\")\n",
    "        print(f\"  Data Type: {stats['dtype']}\")\n",
    "    else:\n",
    "        print(f\"Error: {stats['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temporal Coverage:\n",
      "============================================================\n",
      "Number of timesteps: 10\n",
      "Time units: days since 2020-01-01\n",
      "Start: 2020-01-01 00:00:00\n",
      "End: 2020-01-10 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Check temporal coverage\n",
    "if sample_file.exists():\n",
    "    print(\"\\nTemporal Coverage:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    temporal = inspector.check_temporal_coverage(str(sample_file))\n",
    "    \n",
    "    if 'error' not in temporal:\n",
    "        print(f\"Number of timesteps: {temporal['num_timesteps']}\")\n",
    "        print(f\"Time units: {temporal['units']}\")\n",
    "        \n",
    "        if 'start_date' in temporal:\n",
    "            print(f\"Start: {temporal['start_date']}\")\n",
    "            print(f\"End: {temporal['end_date']}\")\n",
    "    else:\n",
    "        print(f\"Error: {temporal['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spatial Coverage:\n",
      "============================================================\n",
      "Latitude: -90.00° to 90.00°\n",
      "Longitude: -180.00° to 180.00°\n",
      "Lat resolution: 9.47°\n",
      "Lon resolution: 12.41°\n"
     ]
    }
   ],
   "source": [
    "# Check spatial coverage\n",
    "if sample_file.exists():\n",
    "    print(\"\\nSpatial Coverage:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    spatial = inspector.check_spatial_coverage(str(sample_file))\n",
    "    \n",
    "    if 'error' not in spatial:\n",
    "        print(f\"Latitude: {spatial['lat_min']:.2f}° to {spatial['lat_max']:.2f}°\")\n",
    "        print(f\"Longitude: {spatial['lon_min']:.2f}° to {spatial['lon_max']:.2f}°\")\n",
    "        \n",
    "        if spatial['lat_resolution']:\n",
    "            print(f\"Lat resolution: {spatial['lat_resolution']:.2f}°\")\n",
    "        if spatial['lon_resolution']:\n",
    "            print(f\"Lon resolution: {spatial['lon_resolution']:.2f}°\")\n",
    "    else:\n",
    "        print(f\"Error: {spatial['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Enrich Search Results\n",
    "\n",
    "Add LLM-generated summaries to search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Model loaded. Embedding dimension: 384\n",
      "Loaded embedding cache: 23 entries\n",
      "Loading existing index...\n",
      "Initialized FAISS index (dim=384)\n",
      "Index loaded: {'total_vectors': 11, 'total_metadata': 11, 'unique_files': 11, 'embedding_dim': 384, 'index_type': 'IndexFlatIP'}\n",
      "Original Search Results:\n",
      "============================================================\n",
      "\n",
      "1. ocean_temp_feb_2023.nc\n",
      "   Score: 0.548\n",
      "   Title: Test temperature data\n",
      "\n",
      "2. ocean_temp_jan_2023.nc\n",
      "   Score: 0.529\n",
      "   Title: Test temperature data\n",
      "\n",
      "3. ocean_temp_mar_2023.nc\n",
      "   Score: 0.524\n",
      "   Title: Test temperature data\n",
      "\n",
      "============================================================\n",
      "Generating LLM Summaries...\n",
      "============================================================\n",
      "\n",
      "Enriched Results:\n",
      "============================================================\n",
      "\n",
      "1. ocean_temp_feb_2023.nc\n",
      "   Score: 0.548\n",
      "   Summary: This dataset contains test temperature readings collected by the Demo Lab, providing a comprehensive record of thermal data across three dimensions. It could be used to inform quality control measures\n",
      "\n",
      "2. ocean_temp_jan_2023.nc\n",
      "   Score: 0.529\n",
      "   Summary: This dataset contains test temperature data collected at Demo Lab, with temperature being the sole variable measured. It could be used for quality control or research purposes in industries where temp\n",
      "\n",
      "3. ocean_temp_mar_2023.nc\n",
      "   Score: 0.524\n",
      "   Summary: This dataset contains temperature readings from various experiments conducted by the Demo Lab, comprising three-dimensional spatial information. It could be used for analyzing and visualizing temperat\n"
     ]
    }
   ],
   "source": [
    "# Search first\n",
    "try:\n",
    "    engine = FAIRSearchEngine(load_existing=True)\n",
    "    results = engine.search(\"ocean temperature\", top_k=3)\n",
    "    \n",
    "    if results:\n",
    "        print(\"Original Search Results:\")\n",
    "        print(\"=\" * 60)\n",
    "        for i, r in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {Path(r['filepath']).name}\")\n",
    "            print(f\"   Score: {r['similarity_score']:.3f}\")\n",
    "            print(f\"   Title: {r.get('title', 'N/A')}\")\n",
    "        \n",
    "        # Enrich with LLM summaries\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Generating LLM Summaries...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        enriched_results = enricher.enrich_search_results(results)\n",
    "        \n",
    "        print(\"\\nEnriched Results:\")\n",
    "        print(\"=\" * 60)\n",
    "        for i, r in enumerate(enriched_results, 1):\n",
    "            print(f\"\\n{i}. {Path(r['filepath']).name}\")\n",
    "            print(f\"   Score: {r['similarity_score']:.3f}\")\n",
    "            \n",
    "            if 'llm_summary' in r:\n",
    "                print(f\"   Summary: {r['llm_summary']}\")\n",
    "    else:\n",
    "        print(\"No results found. Index some data first.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"No index found. Run indexing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Compare With/Without Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created minimal file: sample_data/test_minimal.nc\n"
     ]
    }
   ],
   "source": [
    "# Create minimal metadata file\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "\n",
    "minimal_file = Path(\"sample_data/test_minimal.nc\")\n",
    "\n",
    "with netCDF4.Dataset(minimal_file, 'w') as ds:\n",
    "    # NO global attributes, only variable name\n",
    "    ds.createDimension('x', 100)\n",
    "    ds.createDimension('y', 100)\n",
    "    ds.createDimension('t', 10)\n",
    "    \n",
    "    var = ds.createVariable('wspd', 'f4', ('t', 'y', 'x'))\n",
    "    var[:] = np.random.randn(10, 100, 100) * 5 + 10\n",
    "\n",
    "print(f\"Created minimal file: {minimal_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without LLM Enrichment:\n",
      "============================================================\n",
      "Title: (none)\n",
      "Institution: (none)\n",
      "Variables: ['wspd']\n",
      "\n",
      "Searchable text length: 74 chars\n"
     ]
    }
   ],
   "source": [
    "# Extract without enrichment\n",
    "extractor = MetadataExtractor()\n",
    "basic_metadata = extractor.extract(minimal_file)\n",
    "\n",
    "print(\"Without LLM Enrichment:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Title: {basic_metadata.get('title') or '(none)'}\")\n",
    "print(f\"Institution: {basic_metadata.get('institution') or '(none)'}\")\n",
    "print(f\"Variables: {list(basic_metadata.get('variables', {}).keys())}\")\n",
    "print(f\"\\nSearchable text length: {len(extractor.create_searchable_text(basic_metadata))} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With LLM Enrichment:\n",
      "============================================================\n",
      "Inferred domain: Atmospheric Science\n",
      "\n",
      "Variable interpretation:\n",
      "  wspd → Wind speed\n",
      "\n",
      "Potential uses:\n",
      "  - Climate modeling\n",
      "  - Weather forecasting\n",
      "\n",
      "Searchable text length: 262 chars (increased by 188\n"
     ]
    }
   ],
   "source": [
    "# Enrich with LLM\n",
    "enriched_metadata = enricher.enrich_metadata(basic_metadata)\n",
    "\n",
    "print(\"\\nWith LLM Enrichment:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'llm_enrichment' in enriched_metadata:\n",
    "    llm_data = enriched_metadata['llm_enrichment']\n",
    "    \n",
    "    print(f\"Inferred domain: {llm_data.get('domain', 'N/A')}\")\n",
    "    print(f\"\\nVariable interpretation:\")\n",
    "    for var, desc in llm_data.get('variable_descriptions', {}).items():\n",
    "        print(f\"  {var} → {desc}\")\n",
    "    \n",
    "    print(f\"\\nPotential uses:\")\n",
    "    for use in llm_data.get('use_cases', []):\n",
    "        print(f\"  - {use}\")\n",
    "    \n",
    "    # Create enriched searchable text\n",
    "    enriched_text = extractor.create_searchable_text(basic_metadata)\n",
    "    enriched_text += \" \" + str(llm_data)\n",
    "    print(f\"\\nSearchable text length: {len(enriched_text)} chars (increased by {len(enriched_text) - len(extractor.create_searchable_text(basic_metadata))}\")\n",
    "else:\n",
    "    print(f\"Error: {enriched_metadata.get('llm_error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Comparison:\n",
      "============================================================\n",
      "Basic extraction: 0.002 seconds\n",
      "LLM enrichment: 13.586 seconds\n",
      "Total with LLM: 13.587 seconds\n",
      "\n",
      "Speed difference: 8636.0x slower with LLM\n",
      "\n",
      "💡 Recommendation:\n",
      "  Consider using LLM enrichment selectively for important datasets\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Time basic extraction\n",
    "start = time.time()\n",
    "basic = extractor.extract(sample_file)\n",
    "basic_time = time.time() - start\n",
    "\n",
    "# Time with LLM enrichment\n",
    "start = time.time()\n",
    "enriched = enricher.enrich_metadata(basic)\n",
    "llm_time = time.time() - start\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Basic extraction: {basic_time:.3f} seconds\")\n",
    "print(f\"LLM enrichment: {llm_time:.3f} seconds\")\n",
    "print(f\"Total with LLM: {basic_time + llm_time:.3f} seconds\")\n",
    "print(f\"\\nSpeed difference: {((basic_time + llm_time) / basic_time):.1f}x slower with LLM\")\n",
    "\n",
    "print(\"\\n💡 Recommendation:\")\n",
    "if llm_time < 5:\n",
    "    print(\"  LLM enrichment is reasonably fast for your use case\")\n",
    "else:\n",
    "    print(\"  Consider using LLM enrichment selectively for important datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use LLM Enrichment\n",
    "\n",
    "### ✅ Good Use Cases\n",
    "- Files with minimal or cryptic metadata\n",
    "- Variable names that are abbreviations\n",
    "- Data from unfamiliar sources\n",
    "- High-value datasets worth extra processing\n",
    "- Interactive exploration (real-time is fine)\n",
    "\n",
    "### ⚠️ Consider Skipping\n",
    "- Large batch processing (1000s of files)\n",
    "- Well-documented CF-compliant files\n",
    "- Time-sensitive indexing workflows\n",
    "- When basic search already works well\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Index without LLM first** - Get basic search working quickly\n",
    "2. **Enrich selectively** - Use LLM for important datasets\n",
    "3. **Cache results** - Store enriched metadata\n",
    "4. **Use local models** - Keep data private\n",
    "5. **Monitor quality** - Verify LLM outputs make sense\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Notebook 99**: See complete workflows with optional enrichment\n",
    "- Try different Ollama models for quality/speed tradeoffs\n",
    "- Integrate enrichment into your indexing pipeline selectively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
